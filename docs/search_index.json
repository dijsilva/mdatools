[
["partial-least-squares-regression.html", "Partial least squares regression", " Partial least squares regression Partial least squares regression is a linear regression method, which uses principles similar to PCA: data is decomposed using latent variables. Because in this case we have two datasets, predictors (\\(X\\)) and responses (\\(Y\\)) we do decomposition for both, computing scores, loadings and residuals: \\(X = TP&#39; + E_x\\), \\(Y = UQ&#39; + E_y\\). In addition to that, orientation of latent variables in PLS is selected to maximize the covariance between the X-scores, \\(T\\), and Y-scores \\(U\\). This approach makes possible to work with datasets where more traditional Multiple Linear Regression fails — when number of variables exceeds number of observations and when X-variables are mutually correlated. But at the end PLS-model is a linear model, where reponse value is a linear combination of predictors, so the main outcome is a vector with regression coefficients. There are two main algorithms for PLS, NIPALS and SIMPLS, in the mdatools only the last one is implemented. PLS model and PLS results objects have a lot of components and performance statistics, which can be visualised via plots. Besides that the implemented pls() method calculates selectivity ratio and VIP scores, which can be used for selection of most important variables. We will discuss most of the methods in this chapter and you can get the full list using ?pls. "],
["models-and-results.html", "Models and results", " Models and results Like we discussed in PCA, matools uses creates two types of objects — a model and a result. Every time you build a PLS model you get a model object. Every time you apply the model to a dataset you get a result object. For PLS, the objects have classes pls and plsres correspondingly. Model calibration Let’s use the same People data and create PLS-model for prediction of Shoesize (column number four) using other 11 variables. As usual we start with preparing datasets (we will also split the data into calibration and test subsets): library(mdatools) data(people) idx = seq(4, 32, 4) X.c = people[-idx, -4, drop = F] y.c = people[-idx, 4] X.t = people[idx, -4] y.t = people[idx, 4, drop = F] So X.c and y.c are predictors and response valurs for calibration subset. Now let’s calibrate the model and show an information about the model object: m = pls(X.c, y.c, 7, scale = T, info = &quot;Shoesize prediction model&quot;) ## Warning in selectCompNum.pls(model): No validation results were found! m = selectCompNum(m, 3) As you can see the procedure is very similar to PCA, here we use 7 latent variables and select 3 first as optimal number. Here is an info for the model object: print(m) ## ## PLS model (class pls) ## ## Call: ## pls.cal(x = x, y = y, ncomp = ncomp, center = center, scale = scale, ## method = method, cv = cv, alpha = alpha, coeffs.ci = coeffs.ci, ## coeffs.alpha = coeffs.alpha, info = info, light = light, ## exclcols = exclcols, exclrows = exclrows, ncomp.selcrit = ncomp.selcrit) ## ## Major fields: ## $ncomp - number of calculated components ## $ncomp.selected - number of selected components ## $coeffs - object (regcoeffs) with regression coefficients ## $xloadings - vector with x loadings ## $yloadings - vector with y loadings ## $weights - vector with weights ## $calres - results for calibration set ## ## Try summary(model) and plot(model) to see the model performance. As expected we see loadings for predictors and responses, matrix with weights, and a special object (regcoeffs) for regression coefficients. The values for regression coefficients are available at m.regcoeffs.values it is an array with dimension nVariables x nComponents x nPredictors. The reason to use the object instead of just an array is mainly for being able to get and plot regression coefficients for different methods. Besides that it is possible to calculate confidence intervals and other statistics for the values using Jack-Knife method (will be show later) which produces extra entities. The regression coefficients can be shown as plot using either function plotRegcoeffs() for the PLS model object or function plot() for the object with regression coefficients. You need to specify for which predictor (if you have more than one y-variable) and which number of components you want to see the coefficients for. By default it shows values for the optimal number of components and first y-variable as it is shown on example below. par(mfrow = c(2, 2)) plotRegcoeffs(m) plotRegcoeffs(m, ncomp = 2) plot(m$coeffs, ncomp = 3, type = &#39;h&#39;, show.labels = T) plot(m$coeffs, ncomp = 2) The model keeps regression coefficients calculated for centered and standardized data, without interceptt, etc. Here are the values for three PLS components. show(m$coeffs$values[, 3, 1]) ## Height Weight Hairleng Age Income ## 0.210411676 0.197646483 -0.138824482 0.026613035 -0.000590693 ## Beer Wine Sex Swim Region ## 0.148917913 0.138138095 -0.138824482 0.223962000 0.010392542 ## IQ ## -0.088658626 You can get the corrected coefficients, which can be applied directly to the raw data by using method getRegcoeffs(): show(getRegcoeffs(m, ncomp = 3)) ## y1 ## Intercept 1.251537e+01 ## Height 8.105287e-02 ## Weight 5.110732e-02 ## Hairleng -5.375404e-01 ## Age 1.147785e-02 ## Income -2.580586e-07 ## Beer 6.521476e-03 ## Wine 1.253340e-02 ## Sex -5.375404e-01 ## Swim 1.164947e-01 ## Region 4.024083e-02 ## IQ -2.742712e-02 And, again, similar to PLS, we see results for calibration set m$loadings[1:4, 1:4] ## NULL One can also notice that the model object has a particular field — calres, which is in fact a PCA result object containing results of applying the model to the calibration set. If we look at the object description we will get the following: print(m$calres) ## ## PLS results (class plsres) ## ## Call: ## plsres(y.pred = yp, y.ref = y.ref, ncomp.selected = object$ncomp.selected, ## xdecomp = xdecomp, ydecomp = ydecomp) ## ## Major fields: ## $ncomp.selected - number of selected components ## $yp - array with predicted y values ## $y - matrix with reference y values ## $rmse - root mean squared error ## $r2 - coefficient of determination ## $slope - slope for predicted vs. measured values ## $bias - bias for prediction vs. measured values ## $ydecomp - decomposition of y values (ldecomp object) ## $xdecomp - decomposition of x values (ldecomp object) And if we want to look at scores, here is the way: m$calres$scores[1:4, 1:4] ## NULL Both model and result objects also have related functions (methods), first of all for visualizing various values (e.g. scores plot, loadings plot, etc.). Some of the functions will be discussed later in this chapter, a full list can be found in help for a proper method. The result object is also created every time you apply a model to a new data. Like in many built-in R methods, method predict() is used in this case. The first argument of the method is always a model object. Here is a PCA example (assuming we have already built the model): res = predict(m, X.t) print(res) ## ## PLS results (class plsres) ## ## Call: ## plsres(y.pred = yp, y.ref = y.ref, ncomp.selected = object$ncomp.selected, ## xdecomp = xdecomp, ydecomp = ydecomp) ## ## Major fields: ## $ncomp.selected - number of selected components ## $yp - array with predicted y values ## $xdecomp - decomposition of x values (ldecomp object) Model validation Any model can be validated with cross-validation or/and test set validation. The validation results are, of course, represented by result objects, which are fields of a model object similar to calres, but with names cvres and testres correspondingly. Here is how to build a PCA model with full cross-validation and test set validation (we will use X.t as test data) at the same time: m = pca(X.c, 7, scale = T, cv = 1, x.test = X.t, info = &quot;PCA model&quot;) m = selectCompNum(m, 5) Parameter cv specifies options for cross-validation. If a numeric value is provided then it will be used as number of segments for random cross-validation, e.g. if cv = 2 cross-validation with two segments will be used. For full cross-validation use cv = 1 like we did in the example above (this is perhaps a bit misleading, but I keep this option for compatability). For more advanced option you can provide a list with name of cross-validation method, number of segments and number of iterations, e.g. cv = list('rand', 4, 4) for running random cross-validation with four segments and four repetitions. And here is the model object info: print(m) ## ## PCA model (class pca) ## ## ## Call: ## pca(x = X.c, ncomp = 7, scale = T, cv = 1, x.test = X.t, info = &quot;PCA model&quot;) ## ## Major fields: ## $loadings - matrix with loadings ## $eigenvals - eigenvalues for components ## $ncomp - number of calculated components ## $ncomp.selected - number of selected components ## $center - values for centering data ## $scale - values for scaling data ## $cv - number of segments for cross-validation ## $alpha - significance level for Q residuals ## $calres - results (scores, etc) for calibration set ## $cvres - results for cross-validation ## $testres - results for test set As you can see we have all three types of results now — calibration (calres), cross-validation (cvres) and test set validation (testres). Let us compare, for example, the explained variance values for the results: var = data.frame(cal = m$calres$expvar, cv = m$cvres$expvar, test = m$testres$expvar) show(round(var, 1)) ## cal cv test ## Comp 1 50.9 39.0 42.1 ## Comp 2 21.7 22.2 17.2 ## Comp 3 14.3 16.3 18.4 ## Comp 4 8.5 14.1 8.7 ## Comp 5 2.4 3.7 5.0 ## Comp 6 1.2 2.3 2.6 ## Comp 7 0.5 0.9 0.6 Every model and every result has a method summary(), which shows some statistics for evaluation of a model performance. Here are some examples. summary(m) ## ## PCA model (class pca) summary ## ## Info: ## PCA model ## ## Eigvals Expvar Cumexpvar ## Comp 1 5.597 50.88 50.88 ## Comp 2 2.386 21.69 72.57 ## Comp 3 1.571 14.29 86.86 ## Comp 4 0.935 8.50 95.36 ## Comp 5 0.260 2.36 97.72 ## Comp 6 0.137 1.24 98.96 ## Comp 7 0.056 0.51 99.47 summary(m$calres) ## ## Summary for PCA results ## ## Selected components: 5 ## ## Expvar Cumexpvar ## Comp 1 50.88 50.88 ## Comp 2 21.69 72.57 ## Comp 3 14.29 86.86 ## Comp 4 8.50 95.36 ## Comp 5 2.36 97.72 ## Comp 6 1.24 98.96 ## Comp 7 0.51 99.47 The same methodology is used for any other method, e.g. PLS or SIMCA. In the next section we will look at how to use plotting functions for models and results. "],
["plotting-methods.html", "Plotting methods", " Plotting methods First of all you can use the methods mdaplot() and mdaplotg() (or any others, e.g. ggplot2) for easy visualisation the results as they all available as matrices with proper names, attributes, etc. In the example below we create several scores and loadings plots. Here I assume that the last model you have created was the one with test set and cross-validation. par(mfrow = c(1, 2)) mdaplot(m$calres$scores, type = &#39;p&#39;, show.labels = T, show.lines = c(0, 0)) mdaplot(m$loadings, type = &#39;p&#39;, show.labels = T, show.lines = c(0, 0)) To simplify this routine, every model and result class also has a number of functions for visualization. Thus for PCA the function list includes scores and loadings plots, explained variance and cumulative explained variance plots, T2 vs. Q residuals and many others. A function that does the same for different models and results has always the same name. For example plotPredictions will show predicted vs. measured plot for PLS model and PLS result, MLR model and MLR result, PCR model and PCR result and so on. The first argument must always be either a model or a result object. The major difference between plots for model and plots for result is following. A plot for result always shows one set of data objects — one set of points, lines or bars. For example predicted vs. measured values for calibration set or scores values for test set and so on. For such plots method mdaplot() is used and you can provide any arguments, available for this method (e.g. color group scores for calibration results). And a plot for a model in most cases shows several sets of data objects, e.g. predicted values for calibration and validation. In this case, a corresponding method uses mdaplotg() and therefore you can adjust the plot using arguments described for this method. Here are some examples for results: par(mfrow = c(2, 2)) plotScores(m$calres, show.labels = T) plotScores(m$calres, c(1, 3), pch = 18, cgroup = X.c[, &#39;Income&#39;], show.labels = T, labels = &#39;indices&#39;) plotResiduals(m$calres, show.labels = T, cgroup = X.c[, &#39;Weight&#39;]) plotVariance(m$calres, type = &#39;h&#39;, show.labels = T, labels = &#39;values&#39;) The color grouping option is not available for the group (model) plots as colors are used there to underline the groups. Now let’s look at similar plots (plus loadings) for a model. par(mfrow = c(2, 2)) plotScores(m, c(1, 3), show.labels = T) plotLoadings(m, c(1, 3), show.labels = T) plotResiduals(m, col = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;)) plotVariance(m, type = &#39;h&#39;, show.labels = T, labels = &#39;values&#39;) Method plot() shows the main four PCA plots as a model (or results) overview. plot(m, show.labels = T) You do not have to care about labels, names, legend and so on, however if necessary you can always change almost anything. See full list of methods available for PCA by ?pca and ?pcares. Support for images As it was described before, images can be used as a source of data for any methods. In this case the results, related to objects/pixels will inherit all necessary attributes and can be show as images as well. In the example below we make a PCA model for the image data from the package and show scores and residuals. data(image) X = mda.im2data(img) m = pca(X) par(mfrow = c(2, 2)) imshow(m$calres$scores) imshow(m$calres$Q) imshow(m$calres$scores, 2) imshow(m$calres$Q, 2) Manual x-values for loading line plot As it was discussed in the previous chapter, you can specify a special attribute, 'xaxis.values' to a dataset, which will be used as manual x-values in bar and line plots. When we create any model and/or results the most important attributes, including this one, are inherited. For example when you make a loading line plot it will be shown using the attribute values. data(simdata) X = simdata$spectra.c attr(X, &#39;xaxis.name&#39;) = &#39;Wavelength, nm&#39; attr(X, &#39;xaxis.values&#39;) = simdata$wavelength m = pca(X, 3) plotLoadings(m, 1:3, type = &#39;l&#39;) Excluding rows and columns From v. 0.8.0 PCA implementation as well as any other method in mdatools can exclude rows and columns from calculations. For example it can be useful if you have some candidates for outliers or do variable selection and do not want to remove rows and columns physically from the data matrix. In this case you can just specify two additional parameters, exclcols and exclrows, using either numbers or names of rows/columns to be excluded. You can also specify a vector with logical values (all TRUEs will be excluded). The excluded rows are not used for creating a model and calculaiton of model’s and results’ performance (e.g. explained variance). However main results (for PCA — scores and residuals) are calculated for these rows as well and set hidden, so you will not see them on plots. You can always e.g. show scores for excluded objects by using show.excluded = TRUE. It is implemented via attributes “known” for plotting methods from mdatools so if you use e.g. ggplot2 you will see all points. The excluded columns are not used for any calculations either, the corresponding results (e.g. loadings or regression coefficients) will have zero values for such columns and be also hidden on plots. Here is a simple example. data(people) m = pca(people, 5, scale = T, exclrows = c(&#39;Lars&#39;, &#39;Federico&#39;), exclcols = &#39;Income&#39;) par(mfrow = c(2, 2)) plotScores(m, show.labels = T) plotScores(m, show.excluded = T, show.labels = T) plotResiduals(m, show.excluded = T, show.labels = T) plotLoadings(m, show.excluded = T, show.labels = T) # show matrix with loadings (look at row Income and attribute &quot;exclrows&quot;) show(m$loadings) ## Comp 1 Comp 2 Comp 3 Comp 4 Comp 5 ## Height -0.386393327 0.10697019 -0.004829174 0.12693029 -0.13128331 ## Weight -0.391013398 0.07820097 0.051916032 0.04049593 -0.14757465 ## Hairleng 0.350435073 -0.11623295 -0.103852349 -0.04969503 -0.73669997 ## Shoesize -0.385424793 0.13805817 -0.069172117 0.01049098 -0.17075488 ## Age -0.103466285 0.18964288 -0.337243182 -0.89254403 -0.02998028 ## Income 0.000000000 0.00000000 0.000000000 0.00000000 0.00000000 ## Beer -0.317356319 -0.38259695 0.044338872 -0.03908064 -0.21419831 ## Wine 0.140711271 0.57861817 -0.059833970 0.12347379 -0.41488773 ## Sex 0.364537185 -0.23838610 0.010818891 0.04025631 -0.18263577 ## Swim -0.377470722 0.04330411 0.008151288 0.18149268 -0.30163601 ## Region 0.140581701 0.60435183 0.040969200 0.15147464 0.17857614 ## IQ 0.009849911 0.09372132 0.927669306 -0.32978247 -0.11815762 ## attr(,&quot;exclrows&quot;) ## [1] 6 ## attr(,&quot;name&quot;) ## [1] &quot;Loadings&quot; ## attr(,&quot;xaxis.name&quot;) ## [1] &quot;Components&quot; Such behavior will help to exclude and include rows and columns interactively, when GUI add-in for mdatools() is available. -->"]
]
