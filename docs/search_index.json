[
["index.html", "Getting started with mdatools for R Introduction", " Getting started with mdatools for R Sergey Kucheryavskiy July 5, 2018 Introduction This is a user guide for mdatools — R package for preprocessing, exploring and analysis of multivariate data. The package provides methods mostly common for Chemometrics. The general idea of the package is to collect the popular chemometric methods and give a similar “user interface” for using them. So if a user knows how to make a model and visualize results for one method, he or she can easily do this for the other methods as well. Usually I update the tutorial when a new version of the package is released, you can track main changes here. All methods implemented in the package were tested using well-known datasets. However, there still could be some bugs, in this case please report to svkucheryavski@gmail.com or use Issues tool at GitHub. You are also very welcome to share your comments and suggestions about the package functionality. "],
["what-is-new.html", "What is new", " What is new What expect from future releases Book Chapter about interval PLS (iPLS) Chapter about randomization test (randtest) Last changes and improvements to book and package 5.07.2018 new chapter about PLS-DA small improvements and corrections to text of other chapters added a short description if new opacity parameter for mdaplot() method 3.04.2018 new chapter about randomized PCA algorithms new chapter about critical limits for residuals in PCA/SIMCA chapter for SIMCA has been updated with description of new methods added a short text about new properties for plotPredictions() method in PLS small corrections and improvements 23.11.2017 corrections in SIMCA part of documentation about using test set 03.08.2017 several minor versions has been released since the last book update (0.8.1-0.8.4) new text describes how to use summary() for regression coefficients text about getRegcoeffs().pls has been updated some updates in plans (see section above) 30.10.2016 fixed a bug in PCA when explained variance was calculated incorrectly for data with excluded rows fixed several issues with SIMCA (cross-validation) and SIMCAM (Cooman’s plot) added a chapter about SIMCA to the tutorial 14.10.2016 The new version (0.8.0) brings a lot of new features, therefore it was decided to rewrite this tutorial completely and start this log from the scratch. Most of the things available in the previous version of the package will work without any changes. But if you have been using functions mdaplot() and mdaplotg() it makes sense to read how the new implementation works and rewrite your code. The use of plotting tools became much simpler and more efficient. The main changes in the package are: added a possibility to assign specific attributes to datasets, which makes plotting easier. added a possibility to exclude (hide) selected rows and columns when create a model. if a data frame has factor columns they will be automatically converted to a set of dummy variables. added several functions to make the operations with datasets containing specific attributes easier. plotting tools (mdaplot(), mdaplotg()) were rewritten to make the use of them easier and more efficient scores and loadings plots now show a percent of explained variance biplot is now available for PCA models (plotBiplot) added support for images, see a specific chapter for details cross-validation procedures were optimized for most of the methods and now takes less time several bug fixes and small improvements Besides that, the tutorial is now available in docs folder of the package repository in GitHub. The tutorial is a static HTML site, which can be used locally without internet connection (start with index.html). However, it is not available from CRAN repository due to CRAN limitations. You can also access the tutorial via GitHub Pages "],
["overview.html", "Overview", " Overview This package was created for an introductory PhD course on Chemometrics given at Department of Chemistry and Bioscience, Aalborg University. Quickly I found out that using R for this course (with all advantages it gives) needs a lot of routine work from students, since most of them were also beginners in R. Of course it is very good for understanding when students get to know e.g. how to calculate explained variance or residuals in PCA manually or make corresponding plots and so on, but for the introductory course these things (as well as numerous typos and small mistakes in a code) take too much time, which can be spent for explaining methods and proper interpretation of results. This is actually also true for everyday using of these methods, most of the routines can be written ones and simply reused with various options. So it was decided to write a package where most widely used chemometric methods for multivariate data analysis are implemented and which gives also a quick and easy-to-use access to results, produced by these methods. First of all numerous plots. Here how it works. Say, we need to make a PCA model for data matrix x with autoscaling. Then make an overview of most important plots and investigate scores and loadings for first three components. The mdatools solution will be: # make a model for autoscaled data with maximum possible number of components m = pca(x, scale = TRUE) # show explained variance plot plotVariance(m) # select optimal number of components (say, 4) for correct calculation of residuals m = selectCompNum(m, 4) # show plots for model overview plot(m) # show scores plot for PC1 and PC3 plotScores(m, c(1, 3)) # show loadings plot for the same components plotLoadings(m, c(1, 3)) # show the loadings as a set of bar plots plotLoadings(m, c(1, 3), type = &#39;h&#39;) Fairly simple, is not it? The other “routine”, which have been taken into account is validation — any model can be cross-validated or validated with a test set. The model object will contain the validation results, which will also appear on all model plots, etc. See the next chapters for details. "],
["what-mdatools-can-do.html", "What mdatools can do?", " What mdatools can do? The package includes classes and functions for analysis, preprocessing and plotting data and results. So far the following methods for analysis are implemented: Principal Component Analysis (PCA) Soft Independent Modelling of Class Analogy (SIMCA) Partial Least Squares regression (PLS) with calculation of VIP scores and Selectivity ratio Partial Least Squares Discriminant Analysis (PLS-DA) Randomization test for PLS regression models Interval PLS for variable selection Preprocessing methods include: Mean centering, standardization and autoscaling Savitzky-Golay filter for smoothing and derivatives Standard Normal Variate for removing scatter effect from spectral data Mutliplicative Scatter Correction for the same issue Normalization of spectra to unit area or unit length Besides that, some extensions for the basic R plotting functionality have been also implemented and allow to do the following: Color grouping of objects with automatic color legend bar. Plot for several groups of objects with automatically calculated axes limits and plot legend. Three built-in color schemes — one is based on Colorbrewer and the other two are jet and grayscale. Very easy-to-use possibility to apply any user defined color scheme. Possibility to show horizontal and vertical lines on the plot with automatically adjusted axes limits. Possibility to extend plotting functionality by using some attributes for datasets. See ?mdatools and next chapters for more details. "],
["how-to-install.html", "How to install", " How to install The package is available from CRAN by usual installing procedure. However due to restrictions in CRAN politics regarding number of submissions (once in 3-4 month) only major releases will be published there. To get the latest release plase use GitHub sources. You can either download a zip-file with the source package and install it using the install.packages command, e.g. if the downloaded file is mdatools_0.9.0.tar.gz and it is located in a current working directory, just run the following: install.packages(&#39;mdatools_0.9.0.tar.gz&#39;) If you have devtools package installed, the following command will install the latest release from the GitHub (do not forget to load the devtools package first): install_github(&#39;svkucheryavski/mdatools&#39;) "],
["datasets-and-plots.html", "Datasets and plots", " Datasets and plots The package uses standard representation of the data in R: data frames, matrices and vectors. However, there are several additional methods and attributes, which make the use of the datasets a bit more more efficient. There is also a support for images. But if you are used to simple datasets and standard procedures and do not want any complications, you can simply skip this chapter. The package also uses its own set of plotting tools, which is a sort of an add-on for the R basic plotting system, extending its possibilities. From this point of view, learning how these tools work will simplify understanding of model plots a lot. The main improvements comparing to the basic plotting system are: Much easier way to make plots with groups of objects (points, lines, bars, etc.) Much easier way of adding legend to the group plots. Much easier way of adding labels to data points, bars, etc. Automatic axis limits when a plot contains several groups of objects. Possibility to color points and lines according to values of a specific numerical variable of a factor. Two built in color pallets and an easy way to use user specific set of colors. Much more! This chapter explains most of the details. "],
["attributes-and-factors.html", "Attributes and factors", " Attributes and factors This section tells how to extend the functionality of the package by using attributes assigned to datasets and how the implemented methods deal with factors. Package specific attributes There are several groups of attributes, which allow to assign names and manual x-values to the datasets, exclude columns and rows from calculations without removing them physically as well as working with images. We will illustrate how to work with most of the attributes by using a simple dataset defined below. It consists of three variables Height, Weight, Shoesize and four records/objects/rows. d = matrix(c(190, 180, 170, 175, 85, 88, 70, 75, 44, 45, 38, 40), ncol = 3) colnames(d) = c(&#39;Height&#39;, &#39;Weight&#39;, &#39;Shoesize&#39;) rownames(d) = c(&#39;Bob&#39;, &#39;Peter&#39;, &#39;Lisa&#39;, &#39;Lena&#39;) d = as.data.frame(d) show(d) ## Height Weight Shoesize ## Bob 190 85 44 ## Peter 180 88 45 ## Lisa 170 70 38 ## Lena 175 75 40 Attributes for plots These attributes will be explained very briefly here, you can find much more details in the next two sections. The idea is to provide some names and values to the data, which can be used later e.g. for making labels and titles on the plots. When dataset is used to create a model (e.g. PCA) all results representing objects (e.g. scores, Q-residuals, T2-residuals, etc.) will inherit the row specific attributes and all results representing objects (e.g. loadings) will inherit column specific attributes. The attributes are following: Attribute Meaning name name of a dataset (used for plot parameter main) xaxis.name name for all data columns (used for plot parameter xlab) yaxis.name name for all data rows (used for plot parameter ylab) xaxis.values a vector of values, which correspond to the columns (e.g. for spectroscopic data it can be wavelength or wavenumbers) yaxis.values a vector of values, which correspond to the rows (e.g. for kinetic data it can be time or temperature of reaction) Here is a very simple example. attr(d, &#39;name&#39;) = &#39;People&#39; attr(d, &#39;xaxis.name&#39;) = &#39;Parameters&#39; attr(d, &#39;yaxis.name&#39;) = &#39;Persons&#39; par(mfrow = c(1, 2)) mdaplot(d, type = &#39;p&#39;) mdaplot(d, type = &#39;l&#39;) See more details in the section about plots. Special methods for data transformations Since data objects in R lose all user specified attributes when e.g. we transpose them or taking a subset it was decided to write several methods, which process attributes correctly. They also adjust indices of excluded rows and columns when user takes a subset or merge two data objects together. When data matrix is transposed the corresponding method will switch the x- and y- attributes. All methods with a brief description are listed in the table below (including the ones already introduces). Method Description mda.show(data) Show data object without excluded elements mda.t(data) Transpose data object mda.cbind(data1, data2, ...) Merge several datasets by columns mda.rbind(data1, data2, ...) Merge several datasets by rows mda.subset(data1, subset, select) Take a subset of data object (subset is numeric indices, names or logical values for rows, select — the same for columns) attrs = mda.getattr(data) Return all user specific attributes from an object data = mda.getattr(data, attrs) Assign user specific attributes to an object Data frames with factors All methods, implemented in the package, work with matrices, therefore, if a user provides data values as data frame, it is converted to matrix. From version 0.8.0 it is also possible to provide data frames with one or several factor columns. In this case all factors will be converted to dummy variables with values –1 and +1. You can also do it manually, by using function prep.df2mat() as this is shown in an example below. Let us first crate a simple data with a factor column. h = c(180, 175, 165, 190, 188) c = c(&#39;Gray&#39;, &#39;Green&#39;, &#39;Gray&#39;, &#39;Green&#39;, &#39;Blue&#39;) d = data.frame(Height = h, Eye.color = c) show(d) ## Height Eye.color ## 1 180 Gray ## 2 175 Green ## 3 165 Gray ## 4 190 Green ## 5 188 Blue And this is the result of converting it to a matrix. d.mat = mda.df2mat(d) show(d.mat) ## Height Blue Gray ## [1,] 180 0 1 ## [2,] 175 0 0 ## [3,] 165 0 1 ## [4,] 190 0 0 ## [5,] 188 1 0 The number of dummy variables by default is the number of levels minus one. You can change this by using argument full = TRUE is it is shown in the example below. d.mat = mda.df2mat(d, full = TRUE) show(d.mat) ## Height Blue Gray Green ## [1,] 180 0 1 0 ## [2,] 175 0 0 1 ## [3,] 165 0 1 0 ## [4,] 190 0 0 1 ## [5,] 188 1 0 0 It is important to have level labels in all factor columns of the same data frame unique, as they are used for names of the dummy variables (e.g. you should not have two factors with the same level name). If a factor is hidden it will be just converted to numeric values and remain excluded from modelling. "],
["simple-plots.html", "Simple plots", " Simple plots As it was already mentioned, mdatools has its own functions for plotting with several extra options not available in basic plot tools. These functions are used to make all plots in the models and results (e.g. scores, loadings, predictions, etc.) therefore it can be useful to spend some time and learn the new features (e.g. coloring data points with a vector of values or using manual ticks for axes). But if you are going to make all plots manually (e.g. using ggplot2) you can skip this and the next sections. In this section we will look at how to make simple plots from your data objects. Simple plots are scatter (type = 'p'), line (type = 'l'), line-scatter (type = 'b'), bar (type = 'h') or errorbar (type = 'e') plots made for a one set of objects. All plots can be created using the same method mdaplot() by providing a whole dataset as a main argument. Depending on a plot type, the method “treats” the data values differently. This table below contains a list of parameters for mdaplot(), which are not available for traditional R plots. In this section we will describe most of the details using simple examples. Parameter Description cgroup a vector of values (same as number of rows in data) used to colorize plot objects with a color gradient colmap colormap for the color gradient (possible values are 'default', 'gray' or a vector with colors) show.colorbar when color grouping is used, mdaplot() shows a colorbar legend, this parameter allows to turn it off show.labels logical parameter showing labels beside plot objects (points, lines, bars, etc). labels parameter telling what to use as labels (by default row names, but can also be indices or manual values) lab.col color for the labels lab.cex font size for the labels (as a scale factor) xticks vector with numeric values to show the x-axis ticks at yticks vector with numeric values to show the y-axis ticks at xticklabels vector with labels (numbers or text) for the x-ticks yticklabels vector with labels (numbers or text) for the y-ticks xlas an integer between 0 and 3 telling at which angle the x-tick labels have to be show ylas an integer between 0 and 3 telling at which angle the y-tick labels have to be show show.axes logical, if TRUE, function will make a new plot, if FALSE, add the plot objects to a previous one show.lines a vector with two numbers — position of horizontal and vertical lines on a plot (e.g. coordinate axes) show.grid logical, show or not a grid show.excluded logical, show or not objects corresponded to the excluded rows opacity opacity of colors in range 0…1 (applied to all colors of current plot) Scatter plots We will use people dataset for illustration how scatter plots work (see ?people for details). data(people) For scatter plots the method takes first two columns of a dataset as x and y vectors. If only one column is available (or data object is a vector), mdaplot() uses it for y-values and generate x-values as an index for each value. par(mfrow = c(2, 2)) mdaplot(people, type = &#39;p&#39;) mdaplot(people[, c(6, 7)], type = &#39;p&#39;) mdaplot(people[, 1], type = &#39;p&#39;, ylab = &#39;Height&#39;) mdaplot(people[1, ], type = &#39;p&#39;, ylab = &#39;&#39;) All parameters, available for the standard points() method will work with mdaplot() as well. Besides that, you can colorize points according to some values using a color gradient. By default, the gradient is generated using one of the diverging color schemes from colorbrewer2.org, but this can be changed using parameter colmap as it is shown below. par(mfrow = c(2, 2)) mdaplot(people, type = &#39;p&#39;, cgroup = people[, &#39;Beer&#39;]) mdaplot(people, type = &#39;p&#39;, cgroup = people[, &#39;Beer&#39;], show.colorbar = F) mdaplot(people, type = &#39;p&#39;, cgroup = people[, &#39;Beer&#39;], colmap = &#39;gray&#39;) mdaplot(people, type = &#39;p&#39;, cgroup = people[, &#39;Beer&#39;], colmap = c(&#39;red&#39;, &#39;yellow&#39;, &#39;green&#39;)) If the vector with values for color grouping is a factor, level labels will be shown on a colorbar legend. g = factor(people[, &#39;Sex&#39;], labels = c(&#39;Male&#39;, &#39;Female&#39;)) par(mfrow = c(1, 2)) mdaplot(people, type = &#39;p&#39;, cgroup = g) mdaplot(people, type = &#39;p&#39;, cgroup = g, colmap = &#39;gray&#39;) Another useful option is adding labels to the data points. By default row names will be taken for the labels but you can specify a parameter 'labels', which can be either a text ('names' or 'indices') or a vector with values to show as labels. Color and size of the labels can be adjusted. par(mfrow = c(2, 2)) mdaplot(people, type = &#39;p&#39;, show.labels = T) mdaplot(people, type = &#39;p&#39;, show.labels = T, labels = &#39;indices&#39;) mdaplot(people, type = &#39;p&#39;, show.labels = T, labels = &#39;names&#39;, lab.col = &#39;black&#39;, lab.cex = 1) mdaplot(people, type = &#39;p&#39;, show.labels = T, labels = paste(&#39;obj&#39;, 1:nrow(people))) The plots work well with the data attributes (names, axis names, etc.). attr(people, &#39;name&#39;) = &#39;People&#39; attr(people, &#39;xaxis.name&#39;) = &#39;Parameters&#39; par(mfrow = c(1, 2)) mdaplot(people, type = &#39;p&#39;, show.labels = T) mdaplot(people, type = &#39;p&#39;, show.labels = T, labels = &#39;indices&#39;) To avoid any problems with arguments when you make a subset, use mda.subset() instead of the traditional ways. As you can see in the example below, if we take first 16 rows, information about excluded objects (as well as all other uder defined arguments, e.g. 'name') disappear and they are show in the plot as normal. But if we use mda.subset() it will take the subset without excluded rows as it is shown below. The subset can be created using logical expressions as well as indices or names of the rows. weight = people[, &#39;Weight&#39;] par(mfrow = c(2, 2)) mdaplot(people[1:16, ], show.labels = T, type = &#39;p&#39;) mdaplot(mda.subset(people, subset = 1:16), show.labels = T, type = &#39;p&#39;) mdaplot(mda.subset(people, subset = c(&#39;Lisa&#39;, &#39;Benito&#39;, &#39;Federico&#39;)), show.labels = T, type = &#39;p&#39;) mdaplot(mda.subset(people, subset = weight &gt; 70), show.labels = T, type = &#39;p&#39;) You can also manually specify axis ticks and tick labels. The labels can be rotated using parameters xlas and ylas, see the examples below. par(mfrow = c(2, 2)) mdaplot(people, xticks = c(165, 175, 185), xticklabels = c(&#39;Small&#39;, &#39;Medium&#39;, &#39;Hight&#39;)) mdaplot(people, yticks = c(55, 70, 85), yticklabels = c(&#39;Light&#39;, &#39;Medium&#39;, &#39;Heavy&#39;)) mdaplot(people, xticks = c(165, 175, 185), xticklabels = c(&#39;Small&#39;, &#39;Medium&#39;, &#39;Hight&#39;), xlas = 2, xlab = &#39;&#39;) mdaplot(people, yticks = c(55, 70, 85), yticklabels = c(&#39;Light&#39;, &#39;Medium&#39;, &#39;Heavy&#39;), ylas = 2, ylab = &#39;&#39;) If both axis labels and rotated axis ticks have to be shown, you can adjust plot margins and position of the label using par() function and mtext() for positioning axis label manually. par(mfrow = c(1, 2)) # change margin for bottom part par(mar = c(6, 4, 4, 2) + 0.1) mdaplot(people, xticks = c(165, 175, 185), xticklabels = c(&#39;Small&#39;, &#39;Medium&#39;, &#39;Hight&#39;), xlas = 2, xlab = &#39;&#39;) mtext(&#39;Height&#39;, side = 1, line = 5) # change margin for left part par(mar = c(5, 6, 4, 1) + 0.1) mdaplot(people, yticks = c(55, 70, 85), yticklabels = c(&#39;Light&#39;, &#39;Medium&#39;, &#39;Heavy&#39;), ylas = 2, ylab = &#39;&#39;) mtext(&#39;Weight&#39;, side = 2, line = 5) There are also a couple of other parameters, allowing to show/hide grid as well as show horizontal and vertical lines on the plot (axes limits will be adjusted correspondingly). par(mfrow = c(1, 2)) mdaplot(people, show.grid = F, show.lines = c(170, 65)) mdaplot(people, show.lines = c(220, NA)) Line plots When line plot is created, the mdatools() shows a line plot for every row. So if data set has more than one row, the plot will show a banch of lines having same properties (color, type, etc). This is particularly useful when working with signals and spectroscopic data. In this subsection we will use simulated UV/Vis spectra from simdata. data(simdata) spectra = simdata$spectra.c conc = simdata$conc.c[, 1] wavelength = simdata$wavelength attr(spectra, &#39;name&#39;) = &#39;UV/Vis spectra&#39; attr(spectra, &#39;xaxis.name&#39;) = &#39;Band index&#39; Here are simple examples of how to make the line plots. par(mfrow = c(2, 1)) mdaplot(spectra, type = &#39;l&#39;) mdaplot(spectra, type = &#39;l&#39;, col = &#39;darkgray&#39;, lty = 2) Most of the parameters described for scatter plots will work for the line plots as well. For example, you can colorise the lines by using a vector with some values (in the example below I use concentration of one of the chemical components). par(mfrow = c(1, 1)) mdaplot(spectra, type = &#39;l&#39;, cgroup = conc) One of the new features, appeared first in version 0.8.0, is a special attribute, allowing to provide manual x-values — 'xaxis.values' (similar parameter for y-values is 'yaxis.values'). In the example below we show the spectra using wavelength in nm and wavenumbers in inverse cm. par(mfrow = c(2, 1)) attr(spectra, &#39;xaxis.name&#39;) = expression(&#39;Wavenumbers, cm&#39;^-1) attr(spectra, &#39;xaxis.values&#39;) = 10^7/wavelength mdaplot(spectra, type = &#39;l&#39;) attr(spectra, &#39;xaxis.name&#39;) = &#39;Wavelength, nm&#39; attr(spectra, &#39;xaxis.values&#39;) = wavelength mdaplot(spectra, type = &#39;l&#39;) When you provide such data to any model methods (e.g. PCA, PLS, etc), then all variable related results (loadings, regression coefficients, etc.) will inherit this attribute and use it for making line plots. Bar and errorbar plots Bar plot is perhaps the simplest as it shows values for the first row of the data as bars. Let us get back to the people data, calculate mean for all variables and show the calculated values as a bar plot (excluding column with Income as it has much bigger values comparing to the others) — in the simplest form as well as with some extra parameters. m = matrix(apply(people, 2, mean), nrow = 1) colnames(m) = colnames(people) m = mda.exclcols(m, &#39;Income&#39;) par(mfrow = c(2, 1)) mdaplot(m, type = &#39;h&#39;) mdaplot(m, type = &#39;h&#39;, xticklabels = colnames(people), col = &#39;red&#39;, show.labels = T, labels = &#39;values&#39;) Errorbar plot always expect data to have two or three rows. The first row is a origin points of the error bars, secod row is the size of the bottom part and the third row is the size of the top part. If data has only two rows the both parts will be symmetric related to the origin. In the example below we show mean and standard deviation of the people data as an error bar. d = rbind(apply(people, 2, mean), apply(people, 2, sd)) rownames(d) = c(&#39;Mean&#39;, &#39;Std&#39;) colnames(d) = colnames(people) attr(d, &#39;name&#39;) = &#39;Statistics&#39; d = mda.exclcols(d, &#39;Income&#39;) par(mfrow = c(2, 1)) mdaplot(d, type = &#39;e&#39;) mdaplot(d, type = &#39;e&#39;, xticklabels = colnames(people), col = &#39;red&#39;) All simple plots can be combined together on the same axes. In this case, first plot is created as usual and all other plots have to be created with option show.axes = F as it is shown below. It must be noted that in this case axes limits have to be set manually when creating the first plot. par(mfrow = c(2, 1)) mdaplot(m, type = &#39;h&#39;, col = &#39;lightgray&#39;, ylim = c(0, 400)) mdaplot(d, type = &#39;e&#39;, show.axes = F, pch = NA) mdaplot(m, type = &#39;b&#39;, ylim = c(0, 400)) mdaplot(d, type = &#39;e&#39;, show.axes = F) In the next section we will discuss plots for several groups of objects (rows). "],
["plots-for-groups-of-objects.html", "Plots for groups of objects", " Plots for groups of objects The package has another method for creating plots, mdaplotg(), which aims at making plots for groups of objects. It can be several groups of points, lines or bars, where evry group has its own attributes, such as color, marker, line type and width, etc. There is a simple criterion to distinguish between the simple and group plots: group plots usually need a legend and simple plots — not. The mdaplotg() method allows to do a lot of things (e.g. split data into groups, add a legend and labels, etc) much easier and this section will show most of the details. I will use the People dataset for most of the examples, so let us load it first, add some attributes, and exclude column with income. data(people) attr(people, &#39;name&#39;) = &#39;People&#39; attr(people, &#39;xaxis.name&#39;) = &#39;Parameters&#39; people = mda.exclcols(people, &#39;Income&#39;) There are three ways to provide data sets for making the group plots. Let’s discuss them first and then talk about some extra features. One matrix or data frame If dataset is a matrix or a data frame, mdaplotg() will make a line, scatter-line or a bar plot, considering every row as a separate group. This can be useful, when, for example, you want to show how explained variance depends on a number of components for calibration and test set, or how loadings for first two components look like. If you want to change any parameters, like pch, lty, lwd, col or similar you need to provide either a vector with value for each group or one value for all groups. Axis limits, ticks, ticklabels, etc. can be defined similarly to the simple plots. Here are some examples. # let&#39;s create a small subset of the people data p = mda.subset(people, subset = c(1, 2, 4), select = c(&#39;Height&#39;, &#39;Weight&#39;, &#39;Shoesize&#39;, &#39;Swim&#39;)) par(mfrow = c(2, 2)) mdaplotg(p, type = &#39;l&#39;) mdaplotg(p, type = &#39;b&#39;) mdaplotg(p, type = &#39;h&#39;, xticks = 1:4) mdaplotg(p, type = &#39;b&#39;, lty = c(1, 2, 1), col = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;), pch = 1, xticklabels = colnames(p)) As you can see, mdaplotg() automatically created the legend and set colors, line parameters, etc. correctly. You can change position of the legend using same names as for basic legend() command from R, or hide it using parameter show.legend = FALSE, as it is shown below. par(mfrow = c(2, 2)) mdaplotg(p, type = &#39;l&#39;, legend.position = &#39;top&#39;) mdaplotg(p, type = &#39;b&#39;, legend.position = &#39;bottomleft&#39;) mdaplotg(p, type = &#39;h&#39;, legend.position = &#39;bottom&#39;) mdaplotg(p, type = &#39;b&#39;, show.legend = F) Group plot also allow to show labels, in this case they can be either values, names or indices of the columns. par(mfrow = c(2, 2)) mdaplotg(p, type = &#39;l&#39;, show.labels = T) mdaplotg(p, type = &#39;b&#39;, show.labels = T, labels = &#39;indices&#39;) mdaplotg(p, type = &#39;h&#39;, show.labels = T, labels = &#39;values&#39;) mdaplotg(p, type = &#39;b&#39;, show.labels = T, labels = &#39;values&#39;) List with matrices or data frames In this case every element of the list will be treated as a separate group. This way allow to make scatter plots as well and line plots with several line in a group. Barplot can be also made but in this case first row from each datasets will be used. If you use names when create the list, the names will be taken as legend labels, otherwise method will look at attribute 'name' for each data set. In the example below we split People data to males and females and show the group plots. sex = people[, &#39;Sex&#39;] m = mda.subset(people, subset = sex == -1) f = mda.subset(people, subset = sex == 1) d = list(male = m, female = f) par(mfrow = c(2, 2)) mdaplotg(d, type = &#39;p&#39;) mdaplotg(d, type = &#39;b&#39;) mdaplotg(d, type = &#39;h&#39;) mdaplotg(d, type = &#39;b&#39;, lty = c(1, 2), col = c(&#39;red&#39;, &#39;blue&#39;), pch = 1) Most of the things described in the previous subsection will work similarly for this case. We will just add a bit more details on how labels and excluded rows are processed for the scatter plots. By default labels are row names or indices. In mdaplotg() you can not provide vector with manual values, so the best way to change them is to assign them as the row names. Indices are unique within each group, so if you have, e.g. three groups of points, there will be three points with index “1”, three with “2”, etc. Use factors to split a dataset into groups One more way to split data set into groups is to provide one or several factor columns using argument groupby. In this case mdaplotg() will find all combinations of the factor levels and split rows of dataset to the corresponding groups. In the example below we use variables Region and Sex to make plots for four groups. sex = factor(people[, &#39;Sex&#39;], labels = c(&#39;M&#39;, &#39;F&#39;)) reg = factor(people[, &#39;Region&#39;], labels = c(&#39;S&#39;, &#39;M&#39;)) groups = data.frame(sex, reg) par(mfrow = c(2, 2)) mdaplotg(people, type = &#39;p&#39;, groupby = groups) mdaplotg(people, type = &#39;l&#39;, groupby = groups) mdaplotg(people, type = &#39;b&#39;, groupby = groups) mdaplotg(people, type = &#39;h&#39;, groupby = groups) All parameters, described before, will work the same way in this case. "],
["working-with-images.html", "Working with images", " Working with images From version 0.8.0 the package also supports images, however they have to be transformed into datasets. The idea is very simple, we keep information about image pixels in an unfolded form, as a matrix, and use attributes width and height to reshape the data when we need to show it as an image. There are three methods that make this procedure easier: mda.im2data(), mda.data2im() and imshow(). The first convert an image (represented as 3-way array) to a data set, second does the opposite and the third takes dataset and shows it as an image. In the code chunk below you will see several examples how the methods work. We will use a dataset image available in the package. It is a 3-way array of numbers, if you want to work with e.g. JPEG, PNG or other standard image files you can load them using specific packages (jpeg, png). data(pellets) # convert image to a data matrix and add some attributed d = mda.im2data(pellets) colnames(d) = c(&#39;Red&#39;, &#39;Green&#39;, &#39;Blue&#39;) attr(d, &#39;name&#39;) = &#39;Image&#39; # show data values mda.show(d, 10) ## Image ## ----- ## Red Green Blue ## [1,] 183 80 101 ## [2,] 191 76 105 ## [3,] 187 73 99 ## [4,] 199 81 113 ## [5,] 198 81 110 ## [6,] 197 84 114 ## [7,] 191 83 109 ## [8,] 193 83 110 ## [9,] 188 83 114 ## [10,] 172 86 115 # show separate channels and the whole image in plots par(mfrow = c(2, 2)) imshow(d, 1) imshow(d, 2) imshow(d, 3) imshow(d, 1:3) By default image for one channels is shown using jet color palette for intensities, but you can also use gray colors, palette from colorbrewer2 as well as your own. par(mfrow = c(2, 2)) imshow(d, 1) imshow(d, 1, colmap = &#39;gray&#39;) imshow(d, 1, colmap = heat.colors(256)) imshow(d, 1, colmap = colorRampPalette(c(&#39;red&#39;, &#39;green&#39;))(256)) You can work with the image values as normal dataset and show scatter, line plots, calculate statistics, etc. par(mfrow = c(1, 2)) mdaplot(d, type = &#39;p&#39;) mdaplot(d, type = &#39;l&#39;) However, it will take some time to show these plots as this image has several hundreds of thousands pixels, a faster alternative can be the use of smoothScatter() function. The mdaplot() has a wrapper for the function (type = 'd'). par(mfrow = c(1, 2)) mdaplot(d, type = &#39;d&#39;) mdaplot(mda.subset(d, select = c(&#39;Red&#39;, &#39;Blue&#39;)), type = &#39;d&#39;) Another useful thing is to set some of the pixels as background. The background pixels are removed from the image dataset physically, there is no way to get them back (in cotrast to excluded rows/pixels). It can be particularly useful when working with e.g. geocorrected hyperspectral images, where, often, many pixels have NA values and there is no need to keep them in memory. To set pixels as background you need to use method mda.setimbg() with either pixel indices or vector with logical values as it is shown below. # original size show(dim(d)) ## [1] 114000 3 # set red epixels as background and show new size d = mda.setimbg(d, d[, &#39;Red&#39;] &gt; 100) show(dim(d)) ## [1] 66471 3 # show image with background pixels par(mfrow = c(1, 2)) imshow(d, 1) imshow(d, 1:3) All image related attributes are inherited by all object/rows related results, e.g. scores, residuals, predicted values and classes, etc. This means if you provide an image to any modelling method, you can visualise the corresponding results also as an image. Some examples will be shown in chapter about PCA "],
["preprocessing.html", "Preprocessing", " Preprocessing The package has several preprocessing methods implemented, mostly for different kinds of spectral data. All functions for preprocessing starts from prefix prep. which makes them easier to find by using code completion. In this chapter a brief description of the methods with several examples will be shown. Autoscaling Autoscaling consists of two steps. First step is centering (or, more precise, mean centering) when center of a data cloud in variable space is moved to an origin. Mathematically it is done by subtracting mean from the data values separately for every column/variable. Second step is scaling og standardization when data values are divided to standard deviation so the variables have unit variance. This autoscaling procedure (both steps) is known in statistics simply as *standardization’. You can also use arbitrary values to center or/and scale the data, in this case use sequence or vector with these values should be provided as an argument for center or scale. R has a built-in function for centering and scaling, scale(). The method prep.autoscale() is actually a wrapper for this function, which is mostly needed to set all user defined attributes to the result (all preprocessing methods will keep the attributes). Here are some examples how to use it: library(mdatools) # get data and exclude column Income data(people) # centering data1 = people data1 = prep.autoscale(data1, center = T, scale = F) # standardization data2 = people data2 = prep.autoscale(data2, center = F, scale = T) # autoscaling data3 = people data3 = prep.autoscale(data3, center = T, scale = T) # centering with median values and standardization data4 = people data4 = prep.autoscale(data4, center = apply(data4, 2, median), scale = T) par(mfrow = c(2, 2)) boxplot(data1, main = &#39;Mean centered&#39;) boxplot(data2, main = &#39;Standardized&#39;) boxplot(data3, main = &#39;Mean centered and standardized&#39;) boxplot(data4, main = &#39;Median centered and standardized&#39;) Correction of spectral baseline Baseline correction methods so far include Standard Normal Variate (SNV) and Multiplicative Scatter Correction (MSC). You can find more methods in the package baseline. SNV is a very simple procedure aiming first of all at remove additive and multiplicative scatter effects from Vis/NIR spectra. It is applied to every individual spectrum by subtracting its average and dividing its standard deviation from all spectral values. Here is an example: # load UV/Vis spectra from Simdata data(simdata) ospectra = simdata$spectra.c attr(ospectra, &#39;xaxis.values&#39;) = simdata$wavelength attr(ospectra, &#39;xaxis.name&#39;) = &#39;Wavelength, nm&#39; # apply SNV and show the spectra pspectra = prep.snv(ospectra) par(mfrow = c(2, 1)) mdaplot(ospectra, type = &#39;l&#39;, main = &#39;Original&#39;) mdaplot(pspectra, type = &#39;l&#39;, main = &#39;after SNV&#39;) Multiplicative Scatter Correction does the same as SNV but in a different way. First it calculates a mean spectrum for the whole set (mean spectrum can be also provided as an extra argument). Then, for each individual spectrum, it makes a line fit for the spectral values and the mean spectrum. The coefficients of the line, intercept and slope, are used to correct the additive and multiplicative effects correspondingly. The prep.msc() function adds the mean spectrum calculated for the original spectral data, to the attributes of the results, so it can be reused later. # apply MSC and and get the preprocessed spectra pspectra = prep.msc(ospectra) # show the result par(mfrow = c(2, 1)) mdaplot(ospectra, type = &#39;l&#39;, main = &#39;Original&#39;) mdaplot(pspectra, type = &#39;l&#39;, main = &#39;after MSC&#39;) Smoothing and derivatives Savitzky-Golay filter is used to smooth signals and calculate derivatives. The filter has three arguments: a width of the filter (width), a polynomial order (porder) and the derivative order (dorder). If the derivative order is zero (default value) only smoothing will be performed. # add random noise to the spectra nspectra = ospectra + 0.025 * matrix(rnorm(length(ospectra)), dim(ospectra)) # apply SG filter for smoothing pspectra = prep.savgol(nspectra, width = 15, porder = 1) # apply SG filter for smoothing and take a first derivative dpspectra = prep.savgol(nspectra, width = 15, porder = 1, dorder = 1) # show results par(mfrow = c(2, 2)) mdaplot(ospectra, type = &#39;l&#39;, main = &#39;Original&#39;) mdaplot(nspectra, type = &#39;l&#39;, main = &#39;Noise added&#39;) mdaplot(pspectra, type = &#39;l&#39;, main = &#39;Smoothing&#39;) mdaplot(dpspectra, type = &#39;l&#39;,main = &#39;First derivative&#39;) "],
["principal-component-analysis.html", "Principal component analysis", " Principal component analysis In this chapter we will discuss how to use PCA method implemented in the mdatools. Besides that, we will use PCA examples to introduce some principles, which are common for most of the other methods (e.g. PLS, SIMCA, PLS-DA, etc.) available in this package. This includes such things as model and result objects, showing performance statistics for models and results, validation, different kinds of plots, and so on. Principal component analysis is one of the methods that decompose a data matrix \\(X\\) into a combination of three matrices: \\(X = TP^T + E\\). Here \\(P\\) is a matrix with unit vectors, defined in the original variables space. The unit vectors form a new basis, which is used to project all data points into. Matrix \\(T\\) contains coordinates of the projections in the new basis and product of the two matrices, \\(TP^T\\) represents the coordinates of projections in original variable space. Matrix \\(E\\) contains residuals — difference between position of projected data points and their original locations. In terms of PCA, the unit-vectors defining the new coordinate space are called loadings and the coordinate axes oriented alongside the loadings are Principal Components (PC). The coordinates of data points projected to the principal components are called scores. There are several other methods, such as Projection Pursuit (PP), Independent Component Analysis (ICA) and some others, that work in a similar way and resulting in the data decomposition shown above. The principal difference among the methods is the way they find the orientation of the unit-vectors. Thus, PCA finds them as directions of maximum variance of data points. In addition to that, all PCA loadings are orthogonal to each other. The PP and ICA use other criteria for the orientation of the basis vectors and e.g. for ICA the vectors are not orthogonal. There are several methods to get loadings for PCA, including Singular Values Decomposition (SVD) and Non-linear Iterative Partial Least Squares (NIPALS). Both methods are implemented in this package and can be selected using method argument of main class pca. By default SVD is used. In addition, one can use randomized version of the two methods, which can be efficient if data matrix contains large amount of rows. This is explained in the last part of this chapter. "],
["models-and-results.html", "Models and results", " Models and results In mdatools, any method for data analysis, such as PCA, PLS regression, SIMCA classification and so on, can create two types of objects — a model and a result. Every time you build a model you get a model object. Every time you apply the model to a dataset you get a result object. Thus for PCA, the objects have classes pca and pcares correspondingly. Each object includes a list with variables (e.g. loadings for model, scores and explained variance for result) and provides a number of methods for visualisation and exploration. Model calibration Let’s see how this works using a simple example — People data. We already used this data when was playing with plots, it consists of 32 objects (persons from Scandinavian and Mediterranean countries, 16 male and 16 female) and 12 variables (height, weight, shoesize, annual income, beer and wine consumption and so on.). More information about the data can be found using ?people. We will first load the data matrix and split it into two subsets as following: library(mdatools) data(people) idx = seq(4, 32, 4) X.c = people[-idx, ] X.t = people[idx, ] So X.c is our calibration subset we are going to use for creating a PCA model and X.t is a subset we will apply the calibrated model to. Now let’s calibrate the model and show an information about the model object: m = pca(X.c, 7, scale = T, info = &quot;People PCA model&quot;) m = selectCompNum(m, 5) Here pca is a function that builds (calibrates) a PCA model and returns the model object. Function selectCompNum allows to select an “optimal” number of components for the model. In our case we calibrate model with 7 principal components (second argument for the function pca()) however, e.g. after investigation of explained variance, we found out that 5 components is optimal. In this case we have two choices. Either recalibrate the model using 5 components or use the model that is calibrated already but “tell” the model that 5 components is the optimal number. In this case the model will keep all results calculated for 7 components but will use optimal number of components when necessary. For example, when showing residuals plot for the model. Or when PCA model is used in SIMCA classification. Finally, function print prints the model object info: print(m) ## ## PCA model (class pca) ## ## ## Call: ## pca(x = X.c, ncomp = 7, scale = T, info = &quot;People PCA model&quot;) ## ## Major fields: ## $loadings - matrix with loadings ## $eigenvals - eigenvalues for components ## $ncomp - number of calculated components ## $ncomp.selected - number of selected components ## $center - values for centering data ## $scale - values for scaling data ## $cv - number of segments for cross-validation ## $alpha - significance level for Q residuals ## $calres - results (scores, etc) for calibration set As you can see, there are no scores, explained variance values, residuals and so on. Because they actually are not part of a PCA model, they are results of applying the model to a calibration set. But loadings, eigenvalues, number of calculated and selected principal components, vectors for centering and scaling the data, number of segments for cross-validation (if used) and significance levels are the model fields: m$loadings[1:4, 1:4] ## Comp 1 Comp 2 Comp 3 Comp 4 ## Height -0.3792846 0.08004057 -0.06676611 0.04512380 ## Weight -0.3817929 0.08533809 -0.08527883 -0.04051629 ## Hairleng 0.3513874 -0.22676635 -0.02273504 0.01575716 ## Shoesize -0.3776985 0.12503739 -0.02117369 0.09929010 One can also notice that the model object has a particular field — calres, which is in fact a PCA result object containing results of applying the model to the calibration set. If we look at the object description we will get the following: print(m$calres) ## ## Results for PCA decomposition (class pcares) ## ## Major fields: ## $scores - matrix with score values ## $T2 - matrix with T2 distances ## $Q - matrix with Q residuals ## $ncomp.selected - selected number of components ## $expvar - explained variance for each component ## $cumexpvar - cumulative explained variance And if we want to look at scores, here is the way: m$calres$scores[1:4, 1:4] ## Comp 1 Comp 2 Comp 3 Comp 4 ## Lars -5.108742 -1.2714943 1.0765871 1.08910438 ## Peter -3.021811 -0.3163758 -0.2958259 -1.36053121 ## Rasmus -2.887335 -0.4428721 0.1231706 -1.15070563 ## Mette 1.116457 -1.3716444 -1.6344512 -0.03803356 Both model and result objects also have related functions (methods), first of all for visualizing various values (e.g. scores plot, loadings plot, etc.). Some of the functions will be discussed later in this chapter, a full list can be found in help for a proper method. The result object is also created every time you apply a model to a new data. Like in many built-in R methods, method predict() is used in this case. The first argument of the method is always a model object. Here is a PCA example (assuming we have already built the model): res = predict(m, X.t) print(res) ## ## Results for PCA decomposition (class pcares) ## ## Major fields: ## $scores - matrix with score values ## $T2 - matrix with T2 distances ## $Q - matrix with Q residuals ## $ncomp.selected - selected number of components ## $expvar - explained variance for each component ## $cumexpvar - cumulative explained variance Model validation Any model can be validated with cross-validation or/and test set validation. The validation results are, of course, represented by result objects, which are fields of a model object similar to calres, but with names cvres and testres correspondingly. Here is how to build a PCA model with full cross-validation and test set validation (we will use X.t as test data) at the same time: m = pca(X.c, 7, scale = T, cv = 1, x.test = X.t, info = &quot;PCA model&quot;) m = selectCompNum(m, 5) Parameter cv specifies options for cross-validation. If a numeric value is provided then it will be used as number of segments for random cross-validation, e.g. if cv = 2 cross-validation with two segments will be used. For full cross-validation use cv = 1 like we did in the example above (this is perhaps a bit misleading, but I keep this option for compatability). For more advanced option you can provide a list with name of cross-validation method, number of segments and number of iterations, e.g. cv = list('rand', 4, 4) for running random cross-validation with four segments and four repetitions. And here is the model object info: print(m) ## ## PCA model (class pca) ## ## ## Call: ## pca(x = X.c, ncomp = 7, scale = T, cv = 1, x.test = X.t, info = &quot;PCA model&quot;) ## ## Major fields: ## $loadings - matrix with loadings ## $eigenvals - eigenvalues for components ## $ncomp - number of calculated components ## $ncomp.selected - number of selected components ## $center - values for centering data ## $scale - values for scaling data ## $cv - number of segments for cross-validation ## $alpha - significance level for Q residuals ## $calres - results (scores, etc) for calibration set ## $cvres - results for cross-validation ## $testres - results for test set As you can see we have all three types of results now — calibration (calres), cross-validation (cvres) and test set validation (testres). Let us compare, for example, the explained variance values for the results: var = data.frame(cal = m$calres$expvar, cv = m$cvres$expvar, test = m$testres$expvar) show(round(var, 1)) ## cal cv test ## Comp 1 54.2 43.1 44.8 ## Comp 2 20.3 21.2 17.2 ## Comp 3 13.1 14.7 17.0 ## Comp 4 7.9 13.0 8.0 ## Comp 5 2.3 3.6 4.4 ## Comp 6 1.1 2.0 2.4 ## Comp 7 0.5 0.8 0.7 Every model and every result has a method summary(), which shows some statistics for evaluation of a model performance. Here are some examples. summary(m) ## ## PCA model (class pca) summary ## ## Info: ## PCA model ## Eigvals Expvar Cumexpvar ## Comp 1 6.509 54.24 54.24 ## Comp 2 2.434 20.28 74.52 ## Comp 3 1.572 13.10 87.62 ## Comp 4 0.946 7.88 95.51 ## Comp 5 0.272 2.27 97.77 ## Comp 6 0.137 1.14 98.92 ## Comp 7 0.058 0.48 99.39 summary(m$calres) ## ## Summary for PCA results ## ## Selected components: 5 ## ## Expvar Cumexpvar ## Comp 1 54.24 54.24 ## Comp 2 20.28 74.52 ## Comp 3 13.10 87.62 ## Comp 4 7.88 95.51 ## Comp 5 2.27 97.77 ## Comp 6 1.14 98.92 ## Comp 7 0.48 99.39 The same methodology is used for any other method, e.g. PLS or SIMCA. In the next section we will look at how to use plotting functions for models and results. "],
["plotting-methods.html", "Plotting methods", " Plotting methods First of all you can use the methods mdaplot() and mdaplotg() (or any others, e.g. ggplot2) for easy visualisation the results as they all available as matrices with proper names, attributes, etc. In the example below we create several scores and loadings plots. Here I assume that the last model you have created was the one with test set and cross-validation. par(mfrow = c(1, 2)) mdaplot(m$calres$scores, type = &#39;p&#39;, show.labels = T, show.lines = c(0, 0)) mdaplot(m$loadings, type = &#39;p&#39;, show.labels = T, show.lines = c(0, 0)) To simplify this routine, every model and result class also has a number of functions for visualization. Thus for PCA the function list includes scores and loadings plots, explained variance and cumulative explained variance plots, T2 distances vs. Q residuals and many others. A function that does the same for different models and results has always the same name. For example, plotPredictions will show predicted vs. measured plot for PLS model and PLS result, MLR model and MLR result, PCR model and PCR result and so on. The first argument must always be either a model or a result object. The major difference between plots for model and plots for result is following. A plot for result always shows one set of data objects — one set of points, lines or bars. For example, predicted vs. measured values for calibration set or scores values for test set and so on. For such plots method mdaplot() is used and you can provide any arguments, available for this method (e.g. color group scores for calibration results). And a plot for a model in most cases shows several sets of data objects, e.g. predicted values for calibration and validation. In this case, a corresponding method uses mdaplotg() and, therefore, you can adjust the plot using arguments described for this method. Here are some examples for results: par(mfrow = c(2, 2)) plotScores(m$calres, show.labels = T) plotScores(m$calres, pch = 18, cgroup = X.c[, &#39;Income&#39;], show.labels = T, labels = &#39;indices&#39;) plotResiduals(m$calres, show.labels = T, cgroup = X.c[, &#39;Weight&#39;]) plotVariance(m$calres, type = &#39;h&#39;, show.labels = T, labels = &#39;values&#39;) The color grouping option is not available for the group (model) plots as colors are used there to underline the groups. Now let’s look at similar plots (plus loadings) for a model. par(mfrow = c(2, 2)) plotScores(m, c(1, 3), show.labels = T) plotLoadings(m, c(1, 3), show.labels = T) plotResiduals(m, col = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;)) plotVariance(m, type = &#39;h&#39;, show.labels = T, labels = &#39;values&#39;) Method plot() shows the main four PCA plots as a model (or results) overview. plot(m, show.labels = T) You do not have to care about labels, names, legend and so on, however if necessary you can always change almost anything. See full list of methods available for PCA by ?pca and ?pcares. Support for images As it was described before, images can be used as a source of data for any methods. In this case the results, related to objects/pixels will inherit all necessary attributes and can be show as images as well. In the example below we make a PCA model for the image data from the package and show scores and residuals. data(pellets) X = mda.im2data(pellets) m = pca(X) par(mfrow = c(2, 2)) imshow(m$calres$scores) imshow(m$calres$Q) imshow(m$calres$scores, 2) imshow(m$calres$Q, 2) Manual x-values for loading line plot As it was discussed in the previous chapter, you can specify a special attribute, 'xaxis.values' to a dataset, which will be used as manual x-values in bar and line plots. When we create any model and/or results the most important attributes, including this one, are inherited. For example when you make a loading line plot it will be shown using the attribute values. data(simdata) X = simdata$spectra.c attr(X, &#39;xaxis.name&#39;) = &#39;Wavelength, nm&#39; attr(X, &#39;xaxis.values&#39;) = simdata$wavelength m = pca(X, 3) plotLoadings(m, 1:3, type = &#39;l&#39;) Excluding rows and columns From v. 0.8.0 PCA implementation as well as any other method in mdatools can exclude rows and columns from calculations. For example it can be useful if you have some candidates for outliers or do variable selection and do not want to remove rows and columns physically from the data matrix. In this case you can just specify two additional parameters, exclcols and exclrows, using either numbers or names of rows/columns to be excluded. You can also specify a vector with logical values (all TRUEs will be excluded). The excluded rows are not used for creating a model and calculaiton of model’s and results’ performance (e.g. explained variance). However main results (for PCA — scores and residuals) are calculated for these rows as well and set hidden, so you will not see them on plots. You can always e.g. show scores for excluded objects by using show.excluded = TRUE. It is implemented via attributes “known” for plotting methods from mdatools so if you use e.g. ggplot2 you will see all points. The excluded columns are not used for any calculations either, the corresponding results (e.g. loadings or regression coefficients) will have zero values for such columns and be also hidden on plots. Here is a simple example. data(people) m = pca(people, 5, scale = T, exclrows = c(&#39;Lars&#39;, &#39;Federico&#39;), exclcols = &#39;Income&#39;) par(mfrow = c(2, 2)) plotScores(m, show.labels = T) plotScores(m, show.excluded = T, show.labels = T) plotResiduals(m, show.excluded = T, show.labels = T) plotLoadings(m, show.excluded = T, show.labels = T) # show matrix with loadings (look at row Income and attribute &quot;exclrows&quot;) show(m$loadings) ## Comp 1 Comp 2 Comp 3 Comp 4 Comp 5 ## Height -0.386393327 0.10697019 -0.004829174 0.12693029 -0.13128331 ## Weight -0.391013398 0.07820097 0.051916032 0.04049593 -0.14757465 ## Hairleng 0.350435073 -0.11623295 -0.103852349 -0.04969503 -0.73669997 ## Shoesize -0.385424793 0.13805817 -0.069172117 0.01049098 -0.17075488 ## Age -0.103466285 0.18964288 -0.337243182 -0.89254403 -0.02998028 ## Income 0.000000000 0.00000000 0.000000000 0.00000000 0.00000000 ## Beer -0.317356319 -0.38259695 0.044338872 -0.03908064 -0.21419831 ## Wine 0.140711271 0.57861817 -0.059833970 0.12347379 -0.41488773 ## Sex 0.364537185 -0.23838610 0.010818891 0.04025631 -0.18263577 ## Swim -0.377470722 0.04330411 0.008151288 0.18149268 -0.30163601 ## Region 0.140581701 0.60435183 0.040969200 0.15147464 0.17857614 ## IQ 0.009849911 0.09372132 0.927669306 -0.32978247 -0.11815762 ## attr(,&quot;exclrows&quot;) ## [1] 6 ## attr(,&quot;name&quot;) ## [1] &quot;Loadings&quot; ## attr(,&quot;xaxis.name&quot;) ## [1] &quot;Components&quot; Such behavior will help to exclude and include rows and columns interactively, when GUI add-in for mdatools() is available. "],
["residuals-and-critical-limits.html", "Residuals and critical limits", " Residuals and critical limits Distance to model and score distance When data objects are being projected to a principal component space, two distances are calculated and stored as a part of PCA results (pcares object). The first is a squared orthogonal Euclidean distance from original position of an object to the PC space, also known as Q-distance or Q-residual (as it is related to residual variation of the data values). The distance shows how well the object is fitted by the PCA model and allows to detect objects that do not follow a commond trend, captured by the PCs. The second distance shows how far a projection of the object to the PC space is from the origin. This distance is also known as Hotelling T2 distance or a score distance. To compute T2 distance scores should be first normalized by dividing them to corresponding singular values. The T2 distance allows to see extreme objects — which are far from the origin. If, for example, we use PCA for making decomposition of height and weight of people, then PC1 will be oriented along a linear trend between the height and weight values common for most of the people. If a person has height and weight within the data range but e.g. is overweighted or underweighted, the corresponding object will have large Q-distance, which indicates that this person does not share the common trend. And, if a person has a ratio between height and weight common for most of the people from the dataset, but, e.g. is too tall, then the corresponding object will have large T2 distance. In mdatools both distances are calculated for all objects of dataset and all possible components. So every distance is represented by nObj x nComp matrix, the first column contains distances for a model with only one component, second — for model with two components, and so on. The distances can be visialised using method plotResiduals() which is available both for PCA results as well as for PCA model. In the case of model the plot shows distances for calibration set, cross-validation and test set (if they were used of course). Here is an example. data(people) m = pca(people, 4, scale = T, cv = 1) par(mfrow = c(2, 2)) plotResiduals(m, main = &#39;Residuals for model&#39;) plotResiduals(m, ncomp = 2, main = &#39;Residuals for model (2 Comp)&#39;) plotResiduals(m$calres, main = &#39;Residuals for calibration&#39;) plotResiduals(m$cvres, main = &#39;Residuals for cross-validated results&#39;) The dashed and dotted lines on the plots are critical limits for extreme objects and for outliers, explained in the next section. They can be hidden/removed from the plots by using additional parameter show.limits = F. You can also change the color, line type and width for the lines by using options lim.col, lim.lty and lim.lwd as it is shown below. par(mfrow = c(1, 2)) plotResiduals(m, show.limits = F) plotResiduals(m, lim.col = c(&#39;red&#39;, &#39;orange&#39;), lim.lwd = c(2, 2), lim.lty = c(1, 2)) It is necessary to provide a vector with two values for each of the argument (first for extreme objects and second for outliers border). Critical limits If PCA model is made for dataset taken from the same poulation, Q and T2 distances can be used to find outliers and extreme objects. One of the ways to do it is to use critical limits for the distances assuming that they follow certain theoretical distribution. Critical limits are also important for SIMCA classification as they directly used for making decision on class belongings. This package implements several methods to compute the limits, which are explained in this section. Critical limits for score distance (T2-distance) The distribution of T2-distances can be well described using Hotelling’s T2 distribution, which, in its turn is based on F-distribution. For given T2 values obtained for N objects using A principal components, the critical limit, T2lim can be computed as follows: \\[T2lim = \\frac{A (N - 1)}{(N - A)} F^{-1}_{1-\\alpha}(A, N-A)\\] Here \\(F^{-1}_{1-\\alpha}(A, N-A)\\) is inverse cumulative distribution function (ICDF) for F-distribution with \\(A\\) and \\(N-A\\) degrees of freedom and \\(\\alpha\\) is a significance level (for example if \\(\\alpha = 0.05\\) we can expect 5% of objects that will have T2-distance larger than this limit). In mdatools the limits are computed based on calibration results and for all possible number of components. The limits are represented as a matrix with four rows and \\(A\\) columns, where the first row contains the limits computed for given alpha parameter (by default alpha = 0.05), second row has similar limits but calculated using another significance level, gamma (by default gamma = 0.01), third row contains mean value for the T2 distances and last row contains degrees of freedom, in this case \\(N-A\\). The limits calculated for gamma are used to detect outliers and shown on residuals plot with dotted line. Here is the limits shown for PCA model computed in the previous example. show(m$T2lim) ## Comp 1 Comp 2 Comp 3 Comp 4 ## Critical limit (alpha = 0.05) 4.159615 6.852714 9.40913 12.01948 ## Outliers limit (gamma = 0.01) 7.529766 11.140048 14.55224 18.04214 ## Mean (u0) 0.968750 1.937500 2.90625 3.87500 ## DoF 31.000000 30.000000 29.00000 28.00000 So, you can see that the critical limit for model with 2 PCs is 6.85 and for model with 4 PCs is 12.02. This is exactly what can be seen as vertical dashed line on the two top residual plots shown in the figure above (left for 4 PCs and right for 2 PCs). Critical limits for orthogonal distance (Q-residuals) The calculation of critical limits for Q-residuals can be done in many different ways. Thus, Swante Wold and Michael Sjöström in the original paper about SIMCA classification suggested that the ratio between Q-distance for a particular object from dataset with \\(M\\)-variables (divided to the corresponding degrees of freedon, \\(M-A\\)) and the variance of all Q-residuals follows F-distribution with \\(M-A\\) and \\((N-A-1)(M-A)\\) degrees of freedon. And, therefore, the limit can be found using ICDF for F-distribution. This method tends to reveal more extreme values than expected, first of all because of \\((N-A-1)(M-A)\\), which is a very big number for modern datasets with hundreds of objects and variables. Several corrections have been proposed since that, most of them are based on using different ways to estimate the degrees of freedom. Using chi-squared distribution On the other hand, several researchers, have found that normalized Q-distances can be well described by chi-squared distribution. The tricky part here is to find proper degrees of freedom as it requires rank of original data matrix, which most of the time is not known. One of the ways to solve this issue is to compute the DF based on the particular Q-values, for example as follows: \\[ DF = 2\\left(\\frac{m_Q}{s_Q}\\right)^2\\] Here \\(m_Q\\) and \\(s_Q\\) are mean and standard deviation for Q-values. In this case, the critical limit for given significance level, \\(\\alpha\\), can be calculated as: \\[Qlim = F^{-1}_{1-\\alpha}(DF) m_Q/DF\\] where \\(F^{-1}_{1-\\alpha}(DF)\\) is ICDF for chi-squared distribution with \\(DF\\) degrees of freedom. Starting from version 0.9.0 this method is implemented in pca class and can be chosen by specifying argument lim.type = 'chisq' as shown in the example below. m = pca(people, 4, scale = T, lim.type = &#39;chisq&#39;) show(m$Qlim) ## Comp 1 Comp 2 Comp 3 Comp 4 ## Critical limit (alpha = 0.05) 9.255125 7.374318 3.648798 1.5596846 ## Outliers limit (gamma = 0.01) 16.876054 16.336489 8.083262 4.1994680 ## Mean (u0) 5.396236 3.223765 1.656619 0.6898182 ## DoF 9.864674 4.147666 4.307598 2.6499084 The matrix with limits for Q-resodials has the same structure as similar matrix for T2-distances The third row contains mean values for Q and the fourth row — degrees of freedom calculated as shown above. Jackson-Mudholkar method Another method has been proposed by Jackson and Mudholkar and is based on using eigenvalues of the residual components thus requires a full decomposition of the data matrix, which makes it computationally heavy if the data is large. Before the version 0.9.0 this method was the only one available for Q limits in mdatools and therefore still remains the default method (although you can specify it explicitly by using option lim.type = 'jm'). Here is an example. m1 = pca(people, 4, scale = T) # default value for lim.type m2 = pca(people, 4, scale = T, lim.type = &#39;jm&#39;) # specify JM as selected method # the calculated limits are the same show(m1$Qlim) ## Comp 1 Comp 2 Comp 3 Comp 4 ## Critical limit (alpha = 0.05) 13.982084 8.915238 4.866821 1.8112567 ## Outliers limit (gamma = 0.01) 21.018106 14.057968 8.284831 2.8278312 ## Mean (u0) 5.396236 3.223765 1.656619 0.6898182 ## DoF 1.000000 1.000000 1.000000 1.0000000 show(m2$Qlim) ## Comp 1 Comp 2 Comp 3 Comp 4 ## Critical limit (alpha = 0.05) 13.982084 8.915238 4.866821 1.8112567 ## Outliers limit (gamma = 0.01) 21.018106 14.057968 8.284831 2.8278312 ## Mean (u0) 5.396236 3.223765 1.656619 0.6898182 ## DoF 1.000000 1.000000 1.000000 1.0000000 Data driven approach for critical limits Later, it was realized (see for example this and this) that chi-squared distribution can be used for both Q- and T2-distances and it is possible to construct a combined limit for both. A. Pomerantsev proposed and then extended an approach, based on similar idea but with a new way for calculation of a combined index, which is now called Data Driven SIMCA (DD-SIMCA). Although the method was developed for SIMCA classification, it is based on calculation of critical limits and therefore can be also used in PCA for similar purposes — detection of extreme objects and outliers based on the residual distances. In this method it is assumed that Q- and T2-values are not independent and there is a combined statistic: \\(DF_Q Q/m_Q + DF_{T^2} T^2 / m_{T^2}\\) which follows chi-squared distribution with \\(DF_Q + DF_{T^2}\\) degrees of freedom. The \\(DF_Q\\) and \\(DF_{T2}\\) as well as the center estimates, \\(m_Q\\) and \\(m_{T^2}\\) are derived from the \\(Q\\) and \\(T^2\\) values computed for the calibration set. There are two ways to estimate them. First is by using classical method of moments, already mentioned: \\[ DF_Q = 2\\left(\\frac{m_Q}{s_Q}\\right)^2\\] \\[ DF_{T^2} = 2\\left(\\frac{m_{T^2})}{s_{T^2})}\\right)^2\\] And the second is based on robust approach utilizing median and inter-quartile range instead of mean and standard deviation (see the paper for details). Both ways are implemented in the package (starting from version 0.9.0) and can be selected by using option lim.type = 'ddmoments' or lim.type = 'ddrobust' correspondingly. Since the acceptance area in this case is not rectangular but triangular, the matrices with calculated limits contain slope (T2lim) and intercept (Qlim) of the border line for the acceptance area instead of individual limits. For example, if we compute the limits as follows: m = pca(people, 4, scale = T, lim.type = &#39;ddmoments&#39;) The matrices will look like this: show(m$Qlim) ## Comp 1 Comp 2 Comp 3 Comp 4 ## Critical limit (alpha = 0.05) 11.346164 19.088550 11.425334 6.3433641 ## Outliers limit (gamma = 0.01) 19.472004 31.770167 18.328590 10.1760634 ## Mean (u0) 5.396236 3.223765 1.656619 0.6898182 ## DoF 10.000000 4.000000 4.000000 3.0000000 show(m$T2lim) ## Comp 1 Comp 2 Comp 3 Comp 4 ## Critical limit (alpha = 0.05) -1.114062 -4.159697 -1.852563 -0.8307488 ## Outliers limit (gamma = 0.01) -1.114062 -4.159697 -1.852563 -0.8307488 ## Mean (u0) 0.968750 1.937500 2.906250 3.8750000 ## DoF 2.000000 10.000000 13.000000 14.0000000 This means that, for example, for two components the line is defined as: \\[Q = -4.16 T^2 + 19.1\\] and for four components as: \\[Q = -0.83 T^2 + 6.34\\] And will look on the residuals plot as follows: par(mfrow = c(1, 2)) plotResiduals(m, ncomp = 2) plotResiduals(m, ncomp = 4) More details about DD-SIMCA method can be also found in chapter, devoted to SIMCA classification. The residuals plot can be also shown for normalized values (\\(Q/m_Q\\) vs. \\(T^2/m_{T^2}\\)) by using option norm = T in the plotResiduals() method. par(mfrow = c(1, 2)) plotResiduals(m, ncomp = 2, norm = T) plotResiduals(m, ncomp = 4, norm = T) "],
["randomized-pca-algorithms.html", "Randomized PCA algorithms", " Randomized PCA algorithms Both SVD and NIPALS are not very efficient when number of rows in dataset is very large (e.g. hundreds of thousands values or even more). Such datasets can be easily obtained in case of for example hyperspectral images. Direct use of the traditional algorithms with such datasets often leads to a lack of memory and long computational time. One of the solution here is to use probabilistic algorithms, which allow to reduce the number of values needed for estimation of principal components. Starting from 0.9.0 one of the probabilistic approach is also implemented in mdatools. The original idea can be found in this paper and some examples on using the approach for PCA analysis of hyperspectral images are described here. The approach is based on calculation of matrix \\(B\\), which has much smaller number of rows comparing to the original data, but captures the action of the data. To produce proper values for \\(B\\), several parameters are used. First of all it is a rank of original data, \\(k\\), if it is known. Second, it is oversampling parameter, \\(p\\), which is important if rank is underestimated. The idea is to use \\(p\\) to overestimate the rank thus making solution more stable. The third parameter, \\(q\\), is a number of iterations needed to make \\(B\\) more robust. Usually using \\(p = 5\\) and \\(q = 1\\) will work well on most of the datasets and, at the same time, will take less time for finding a solution comparing with conventional methods. By default, \\(k\\) is set to number of components, used for PCA decomposition. The example below uses two methods (classic SVD and randomized SVD) for PCA decomposition of 100 000 x 300 dataset and compares time and main outcomes (loadings and explained variance). # create a dataset as a linear combination of three sin curves with random &quot;concentrations&quot; n = 100000 X = seq(0, 29.9, by = 0.1) S = cbind(sin(X), sin(10 * X), sin(5 * X)) C = cbind(runif(n, 0, 1), runif(n, 0, 2), runif(n, 0, 3)) D = C %*% t(S) D = D + matrix(runif(300 * n, 0, 0.5), ncol = 300) show(dim(D)) ## [1] 100000 300 # conventional SVD t1 = system.time({m1 = pca(D, ncomp = 2)}) show(t1) ## user system elapsed ## 50.190 2.300 54.385 # randomized SVD with p = 5 and q = 1 t2 = system.time({m2 = pca(D, ncomp = 2, rand = c(5, 1))}) show(t2) ## user system elapsed ## 28.555 2.127 32.162 # compare variances summary(m1) ## ## PCA model (class pca) summary ## ## Info: ## ## Eigvals Expvar Cumexpvar ## Comp 1 112.401 62.09 62.09 ## Comp 2 49.869 27.55 89.64 summary(m2) ## ## PCA model (class pca) summary ## ## Info: ## ## ## Parameters for randomized algorithm: q = 5, p = 1 ## Eigvals Expvar Cumexpvar ## Comp 1 112.401 62.09 62.09 ## Comp 2 49.869 27.55 89.64 # compare loadings show(m1$loadings[1:10, ]) ## Comp 1 Comp 2 ## [1,] 4.186171e-05 -7.781134e-06 ## [2,] 3.904258e-02 -6.903224e-02 ## [3,] 6.853590e-02 -7.490861e-02 ## [4,] 8.144432e-02 -1.209039e-02 ## [5,] 7.424440e-02 6.137773e-02 ## [6,] 4.893136e-02 7.813859e-02 ## [7,] 1.148679e-02 2.273533e-02 ## [8,] -2.861022e-02 -5.353235e-02 ## [9,] -6.177736e-02 -8.053437e-02 ## [10,] -7.976901e-02 -3.324322e-02 show(m2$loadings[1:10, ]) ## Comp 1 Comp 2 ## [1,] 4.186171e-05 7.781134e-06 ## [2,] 3.904258e-02 6.903224e-02 ## [3,] 6.853590e-02 7.490861e-02 ## [4,] 8.144432e-02 1.209039e-02 ## [5,] 7.424440e-02 -6.137773e-02 ## [6,] 4.893136e-02 -7.813859e-02 ## [7,] 1.148679e-02 -2.273533e-02 ## [8,] -2.861022e-02 5.353235e-02 ## [9,] -6.177736e-02 8.053437e-02 ## [10,] -7.976901e-02 3.324322e-02 As you can see the explained variance values, eigenvalues and loadings are identical in the two models and the second method is about twice faster. It is possible to make PCA decomposition even faster if only loadings and scores are needed. In this case you can use method pca.run() and skip other steps, like calculation of residuals, variances, critical limits and so on. But in this case data matrix must be centered (and scaled if necessary) manually prior to the decomposition. Here is an example using the data generated in previous code. D = scale(D, center = T, scale = F) # conventional SVD t1 = system.time({P1 = pca.run(D, method = &#39;svd&#39;, ncomp = 2)}) show(t1) ## user system elapsed ## 22.881 0.274 23.253 # randomized SVD with p = 5 and q = 1 t2 = system.time({P2 = pca.run(D, method = &#39;svd&#39;, ncomp = 2, rand = c(5, 1))}) show(t2) ## user system elapsed ## 1.783 0.041 1.825 # compare loadings show(P1$loadings[1:10, ]) ## [,1] [,2] ## [1,] 4.186171e-05 -7.781134e-06 ## [2,] 3.904258e-02 -6.903224e-02 ## [3,] 6.853590e-02 -7.490861e-02 ## [4,] 8.144432e-02 -1.209039e-02 ## [5,] 7.424440e-02 6.137773e-02 ## [6,] 4.893136e-02 7.813859e-02 ## [7,] 1.148679e-02 2.273533e-02 ## [8,] -2.861022e-02 -5.353235e-02 ## [9,] -6.177736e-02 -8.053437e-02 ## [10,] -7.976901e-02 -3.324322e-02 show(P2$loadings[1:10, ]) ## [,1] [,2] ## [1,] 4.186171e-05 7.781134e-06 ## [2,] 3.904258e-02 6.903224e-02 ## [3,] 6.853590e-02 7.490861e-02 ## [4,] 8.144432e-02 1.209039e-02 ## [5,] 7.424440e-02 -6.137773e-02 ## [6,] 4.893136e-02 -7.813859e-02 ## [7,] 1.148679e-02 -2.273533e-02 ## [8,] -2.861022e-02 5.353235e-02 ## [9,] -6.177736e-02 8.053437e-02 ## [10,] -7.976901e-02 3.324322e-02 As you can see the loadings are still the same but the probabilistic algorithm is about 15 times faster. "],
["partial-least-squares-regression.html", "Partial least squares regression", " Partial least squares regression Partial least squares regression is a linear regression method, which uses principles similar to PCA: data is decomposed using latent variables. Because in this case we have two datasets, predictors (\\(X\\)) and responses (\\(Y\\)) we do decomposition for both, computing scores, loadings and residuals: \\(X = TP^T + E_x\\), \\(Y = UQ^T + E_y\\). In addition to that, orientation of latent variables in PLS is selected to maximize the covariance between the X-scores, \\(T\\), and Y-scores \\(U\\). This approach makes possible to work with datasets where more traditional Multiple Linear Regression fails — when number of variables exceeds number of observations and when X-variables are mutually correlated. But at the end PLS-model is a linear model, where response value is a linear combination of predictors, so the main outcome is a vector with regression coefficients. There are two main algorithms for PLS, NIPALS and SIMPLS, in the mdatools only the last one is implemented. PLS model and PLS results objects have a lot of components and performance statistics, which can be visualised via plots. Besides that the implemented pls() method calculates selectivity ratio and VIP scores, which can be used for selection of most important variables. We will discuss most of the methods in this chapter and you can get the full list using ?pls. "],
["models-and-results-1.html", "Models and results", " Models and results Like we discussed in PCA, matools creates two types of objects — a model and a result. Every time you build a PLS model you get a model object. Every time you apply the model to a dataset you get a result object. For PLS, the objects have classes pls and plsres correspondingly. Model calibration Let’s use the same People data and create a PLS-model for prediction of Shoesize (column number four) using other 11 variables as predictors. As usual, we start with preparing datasets (we will also split the data into calibration and test subsets): library(mdatools) data(people) idx = seq(4, 32, 4) X.c = people[-idx, -4] y.c = people[-idx, 4, drop = F] X.t = people[idx, -4] y.t = people[idx, 4, drop = F] So X.c and y.c are predictors and response values for calibration subset. Now let’s calibrate the model and show an information about the model object: m = pls(X.c, y.c, 7, scale = T, info = &quot;Shoesize prediction model&quot;) ## Warning in selectCompNum.pls(model): No validation results were found! m = selectCompNum(m, 3) As you can see, the procedure is very similar to PCA, here we use 7 latent variables and select 3 first as an optimal number. Here is an info for the model object: print(m) ## ## PLS model (class pls) ## ## Call: ## pls.cal(x = x, y = y, ncomp = ncomp, center = center, scale = scale, ## method = method, cv = cv, alpha = alpha, coeffs.ci = coeffs.ci, ## coeffs.alpha = coeffs.alpha, info = info, light = light, ## exclcols = exclcols, exclrows = exclrows, ncomp.selcrit = ncomp.selcrit) ## ## Major fields: ## $ncomp - number of calculated components ## $ncomp.selected - number of selected components ## $coeffs - object (regcoeffs) with regression coefficients ## $xloadings - vector with x loadings ## $yloadings - vector with y loadings ## $weights - vector with weights ## $calres - results for calibration set ## ## Try summary(model) and plot(model) to see the model performance. As expected, we see loadings for predictors and responses, matrix with weights, and a special object (regcoeffs) for regression coefficients. The values for regression coefficients are available in m.coeffs.values, it is an array with dimension nVariables x nComponents x nPredictors. The reason to use the object instead of just an array is mainly for being able to get and plot regression coefficients for different methods. Besides that, it is possible to calculate confidence intervals and other statistics for the coefficients using Jack-Knife method (will be shown later), which produces extra entities. The regression coefficients can be shown as plot using either function plotRegcoeffs() for the PLS model object or function plot() for the object with regression coefficients. You need to specify for which predictor (if you have more than one y-variable) and which number of components you want to see the coefficients for. By default it shows values for the optimal number of components and first y-variable as it is shown on example below. par(mfrow = c(2, 2)) plotRegcoeffs(m) plotRegcoeffs(m, ncomp = 2) plot(m$coeffs, ncomp = 3, type = &#39;h&#39;, show.labels = T) plot(m$coeffs, ncomp = 2) The model keeps regression coefficients, calculated for centered and standardized data, without intercept, etc. Here are the values for three PLS components. show(m$coeffs$values[, 3, 1]) ## Height Weight Hairleng Age Income ## 0.210411676 0.197646483 -0.138824482 0.026613035 -0.000590693 ## Beer Wine Sex Swim Region ## 0.148917913 0.138138095 -0.138824482 0.223962000 0.010392542 ## IQ ## -0.088658626 You can see a summary for the regression coefficients object by calling function summary() for the object m$coeffs like it is show below. By default it shows only estimated regression coefficients for the selected y-variable and number of components. However, if you employ Jack-Knifing (see section Variable selection below), the object with regression coefficients will also contain some statistics, including standard error, p-value (for test if the coefficient is equal to zero in populatio) and confidence interval. All statistics in this case will be shown automatically with summary(). summary(m$coeffs) ## Regression coefficients for Shoesize ## ------------------------------------ ## Height Weight Hairleng Age Income ## 0.176077659 0.175803980 -0.164627444 0.046606027 0.059998121 ## Beer Wine Sex Swim Region ## 0.133136867 0.002751573 -0.164627444 0.173739533 -0.031357608 ## IQ ## -0.003353428 summary(m$coeffs, ncomp = 3) ## Regression coefficients for Shoesize ## ------------------------------------ ## Height Weight Hairleng Age Income ## 0.210411676 0.197646483 -0.138824482 0.026613035 -0.000590693 ## Beer Wine Sex Swim Region ## 0.148917913 0.138138095 -0.138824482 0.223962000 0.010392542 ## IQ ## -0.088658626 You can also get the corrected coefficients, which can be applied directly to the raw data (without centering and standardization), by using method getRegcoeffs(): show(getRegcoeffs(m, ncomp = 3)) ## Shoesize ## Intercept 1.251537e+01 ## Height 8.105287e-02 ## Weight 5.110732e-02 ## Hairleng -5.375404e-01 ## Age 1.147785e-02 ## Income -2.580586e-07 ## Beer 6.521476e-03 ## Wine 1.253340e-02 ## Sex -5.375404e-01 ## Swim 1.164947e-01 ## Region 4.024083e-02 ## IQ -2.742712e-02 ## attr(,&quot;name&quot;) ## [1] &quot;Regression coefficients for Shoesize&quot; It also returns all statistics in case if Jack-Knifing was applied. Similar to PCA, model object may contain three fields for results obtained using calibration set (calres), cross-validation (cvres) and test set validation (testres). All three have class plsres, here is how calres looks like: print(m$calres) ## ## PLS results (class plsres) ## ## Call: ## plsres(y.pred = yp, y.ref = y.ref, ncomp.selected = object$ncomp.selected, ## xdecomp = xdecomp, ydecomp = ydecomp) ## ## Major fields: ## $ncomp.selected - number of selected components ## $yp - array with predicted y values ## $y - matrix with reference y values ## $rmse - root mean squared error ## $r2 - coefficient of determination ## $slope - slope for predicted vs. measured values ## $bias - bias for prediction vs. measured values ## $ydecomp - decomposition of y values (ldecomp object) ## $xdecomp - decomposition of x values (ldecomp object) The xdecomp and ydecomp are objects similar to pcares, they contain scores, residuals and variances for decomposition of X and Y correspondingly. print(m$calres$xdecomp) ## ## Results of data decomposition (class ldecomp) ## ## Major fields: ## $scores - matrix with score values ## $T2 - matrix with T2 distances ## $Q - matrix with Q residuals ## $ncomp.selected - selected number of components ## $expvar - explained variance for each component ## $cumexpvar - cumulative explained variance Other fields are mostly various performance statistics, including slope, coefficient of determination (R2), bias, and root mean squared error (RMSE). Besides that, the results also include reference y-values and array with predicted y-values. The array has dimension nObjects x nComponents x nResponses. PLS predictions for a new set can be obtained using method predict: res = predict(m, X.t, y.t) print(res) ## ## PLS results (class plsres) ## ## Call: ## plsres(y.pred = yp, y.ref = y.ref, ncomp.selected = object$ncomp.selected, ## xdecomp = xdecomp, ydecomp = ydecomp) ## ## Major fields: ## $ncomp.selected - number of selected components ## $yp - array with predicted y values ## $y - matrix with reference y values ## $rmse - root mean squared error ## $r2 - coefficient of determination ## $slope - slope for predicted vs. measured values ## $bias - bias for prediction vs. measured values ## $ydecomp - decomposition of y values (ldecomp object) ## $xdecomp - decomposition of x values (ldecomp object) Model validation Validation is implemented similar to PCA, the only difference is that you need to provide two datasets for a test set — one for predictors (x.test) and one for response (y.test) values. Cross-validation is very important for PLS as it helps to find optimal number of PLS components (so test set performance is more fair as in this case you do not use test set for optimization). Therefore, it is always recommended to use cross-validation. You probably have noticed a small warning we got when created the first PLS model in this chapter: m = pls(X.c, y.c, 7, scale = T, info = &quot;Shoesize prediction model&quot;) ## Warning in selectCompNum.pls(model): No validation results were found! When you create a model, it tries to select optimal number of components automatically (which, of course, you can always change later). To do that, the method uses RMSE values, calculated for different number of components and cross-validation predictions. So, if we do not use cross-validation, it warns use about this. There are two different ways/criteria for automatic selection. One is using first local minimum on the RMSE plot and second is so called Wold criterion, based on a ratio between PRESS values for current and next component. You can select which criterion to use by specifying parameter ncomp.selcrit (either 'min' or 'wold') as it is shown below. m1 = pls(X.c, y.c, 7, scale = T, cv = 1, ncomp.selcrit = &#39;min&#39;) show(m1$ncomp.selected) ## [1] 5 m2 = pls(X.c, y.c, 7, scale = T, cv = 1, ncomp.selcrit = &#39;wold&#39;) show(m2$ncomp.selected) ## [1] 4 And here are the RMSE plots (they are identical of course): par(mfrow = c(1, 2)) plotRMSE(m1) plotRMSE(m2) Parameter cv has the same format as for PCA. If it is a number, it will be used as number of segments for random cross-validation, e.g. if cv = 2 cross-validation with two segments will be carried out. For full cross-validation use cv = 1 like we did in the example above. For more advanced option you can provide a list with name of cross-validation method, number of segments and number of iterations, e.g. cv = list('rand', 4, 4) for running random cross-validation with four segments and four repetitions or cv = list('ven', 8) for systematic split into eight segments (venetian blinds). Method summary() for model shows performance statistics calculated using optimal number of components for each of the results. summary(m1) ## ## PLS model (class pls) summary ## ## Performance and validation: ## Number of selected components: 5 ## X cumexpvar Y cumexpvar RMSE Slope Bias RPD ## Cal 97.64 98.19 0.521 0.98 0e+00 7.59 ## CV 92.90 96.22 0.753 0.98 -2e-04 5.26 If you want more details run summary() for one of the result objects. summary(m1$calres) ## ## PLS regression results (class plsres) summary ## ## Response variable Shoesize: ## X expvar X cumexpvar Y expvar Y cumexpvar RMSE Slope Bias RPD ## Comp 1 50.505 50.505 93.779 93.779 0.966 0.938 0 4.1 ## Comp 2 20.979 71.484 2.926 96.705 0.703 0.967 0 5.6 ## Comp 3 8.667 80.151 0.917 97.622 0.597 0.976 0 6.6 ## Comp 4 5.847 85.998 0.479 98.101 0.534 0.981 0 7.4 ## Comp 5 11.642 97.640 0.088 98.189 0.521 0.982 0 7.6 ## Comp 6 0.495 98.135 0.347 98.536 0.468 0.985 0 8.4 ## Comp 7 0.442 98.577 0.202 98.738 0.435 0.987 0 9.1 There is no column for R2 as Y cumexpvar values are the same. "],
["plotting-methods-1.html", "Plotting methods", " Plotting methods Plotting methods, again, work similar to PCA, so in this section we will look more detailed on the available methods instead of on how to customize them. PLS has a lot of different results and much more possible plots. Here is a list of methods, which will work both for a model and for a particular results. Methods for summary statistics. Methods Description plotRMSE(obj, ny = 1, ...) RMSE values vs. number of components in a model plotXVariance(obj) explained variance for X decomposition for each component plotXCumVariance(obj) same as above but cumulative plotYVariance(obj) explained variance for Y decomposition for each component plotYCumVariance(obj) same as above but cumulative Here and in some other methods parameter ny is used to specify which y-variable you want to see a plot for (if y is multivariate). Methods for objects. Methods Description plotPredictions(obj, ny = 1, ncomp) plot with predicted vs. measured (reference) y-values plotXScores(obj, comp) scores for decompositon of X (similar to PCA) plotYScores(obj, comp) scores for decompositon of y (similar to PCA) plotXResiduals(obj, ncomp) residuals for decompositon of X (similar to PCA) plotYResiduals(obj, ncomp) residuals for y vs. real (reference) y-values plotXScores(obj, ncomp) Y-scores vs. X-scores for a particular PLS component. Parameter comp allows to provide a number of selected components (one or several) to show the plot for, while parameter ncomp assume that only one number is expected (number of components in a model or individual component). So if e.g. you created model for five components and selected three, you can also see, for example, prediction plot if you use only one or four components. Here is an example for m1 model: par(mfrow = c(1, 2)) plotPredictions(m1) plotPredictions(m1, ncomp = 1) By the way, when plotPredictions() is made for results object, you can show performance statistics on the plot (implemented in 0.9.0): par(mfrow = c(2, 2)) plotPredictions(m1$calres) plotPredictions(m1$calres, ncomp = 2) plotPredictions(m1$calres, show.stat = T) plotPredictions(m1$calres, ncomp = 2, show.stat = T) The plots for variables are available only for a model object and include: Methods Description plotXLoadings(obj, comp) loadings plot for decompositon of X plotYLoadings(obj, comp) loadings plot for decompositon of y plotWeights(obj, comp) plot with weights (W) for PLS decomposition plotRegcoeffs(obj, ny, ncomp) plot with regression coefficients plotVIPScores(obj, ny) VIP scores plot plotSelectivityRatio(obj, ny, ncomp) Selectivity ratio plot And, of course, both model and result objects have method plot() for giving an overview. plot(m1) Excluding rows and columns From v. 0.8.0 PCA implementation as well as any other method in mdatools can exclude rows and columns from calculations. The implementation works similar to what was described for PCA. For example it can be useful if you have some candidates for outliers or do variable selection and do not want to remove rows and columns physically from the data matrix. In this case you can just specify two additional parameters, exclcols and exclrows, using either numbers or names of rows/columns to be excluded. You can also specify a vector with logical values (all TRUEs will be excluded). The excluded rows are not used for creating a model and calculation of model’s and results’ performance (e.g. explained variance). However main results (for PLS — scores, predictions, residuals) are calculated for these rows as well and set hidden, so you will not see them on plots. You can always e.g. show scores for excluded objects by using show.excluded = TRUE. It is implemented via attributes “known” for plotting methods from mdatools so if you use e.g. ggplot2 you will see all points. The excluded columns are not used for any calculations either, the corresponding results (e.g. loadings, weights or regression coefficients) will have zero values for such columns and be also hidden on plots. "],
["variable-selection.html", "Variable selection", " Variable selection PLS calculates several statistics, which can be used to select most important (or remove least important) variables in order to improve performance and make model simpler. The first two are VIP-scores (variables important for projection) and Selectivity ratio. All details and theory can be found e.g. here. Both parameters can be shown as plots and as vector of values for a selected y-variable. Selectivity ration is calculated for all possible components in a model, but VIP scores (due to computational time) only for selected number of components and are recalculated every time you change number of optimal components using selectCompNum() method. Here are some plots. par(mfrow = c(2, 2)) plotVIPScores(m1, type = &#39;h&#39;, show.labels = T) plotSelectivityRatio(m1, type = &#39;b&#39;, show.labels = T) plotSelectivityRatio(m1, ncomp = 1, type = &#39;h&#39;, show.labels = T) plotSelectivityRatio(m1, ncomp = 2, type = &#39;h&#39;, show.labels = T) In the example below, I create two other PLS models by excluding variables with VIP score or selectivity ratio below a threshold (I use 0.5 and 1 correspondingly) and show the performance for both. m3 = pls(X.c, y.c, 5, scale = T, cv = 1, exclcols = getVIPScores(m1, ncomp = 2) &lt; 0.5) summary(m3) ## ## PLS model (class pls) summary ## ## Performance and validation: ## Number of selected components: 4 ## X cumexpvar Y cumexpvar RMSE Slope Bias RPD ## Cal 85.29 98.15 0.527 0.98 0.0000 7.50 ## CV 80.53 96.34 0.741 0.97 -0.0382 5.34 m4 = pls(X.c, y.c, 5, scale = T, cv = 1, exclcols = getSelectivityRatio(m1, ncomp = 2) &lt; 1) summary(m4) ## ## PLS model (class pls) summary ## ## Performance and validation: ## Number of selected components: 1 ## X cumexpvar Y cumexpvar RMSE Slope Bias RPD ## Cal 86.80 94.97 0.868 0.95 0.0000 4.56 ## CV 84.37 93.91 0.955 0.94 0.0034 4.14 Another way is make an inference about regression coefficients and calculate confidence intervals and p-values for each variable. This can be done usine Jack-Knife approach, when model is cross-validated using efficient number of segments (at least ten) and statistics are calculated using the distribution of regression coefficient values obtained for each step. There are two parameters, coeffs.ci and coeffs.alpha, first is to select the method (so far only Jack-Knife is available, the value is 'jk') and second is a level of significance for computing confidence intervals (by default is 0.1). Here is an example. mjk = pls(X.c, y.c, 7, scale = T, coeffs.ci = &#39;jk&#39;, coeffs.alpha = 0.05) If number of segments is not specified as in the example above, full cross-validation will be used. The statistics are calculated for each y-variable and each available number of components. When you show a plot for regression coefficients, confidence interval will be shown automatically. You can changes this by using parameter show.ci = FALSE. par(mfrow = c(2, 2)) plotRegcoeffs(mjk, type = &#39;h&#39;, show.labels = T) plotRegcoeffs(mjk, ncomp = 2, type = &#39;h&#39;, show.labels = T) plotRegcoeffs(mjk, type = &#39;h&#39;, show.labels = T, show.ci = F) plotRegcoeffs(mjk, ncomp = 2, type = &#39;h&#39;, show.labels = T, show.ci = F) Calling function summary() for regression coefficients allows to get all calculated statistics. summary(mjk$coeffs, ncomp = 2) ## Regression coefficients for Shoesize ## ------------------------------------ ## Estimated coefficients t-value SE p-value 95% CI (lo) ## Height 0.19357436 25.6 0.007507308 0.000 0.17804431 ## Weight 0.18935489 24.6 0.007629829 0.000 0.17357139 ## Hairleng -0.17730737 -17.3 0.010263098 0.000 -0.19853820 ## Age 0.02508113 0.9 0.026825735 0.358 -0.03041213 ## Income 0.01291497 0.4 0.034034881 0.703 -0.05749155 ## Beer 0.11441573 6.3 0.018145181 0.000 0.07687956 ## Wine 0.08278735 2.7 0.030127820 0.012 0.02046321 ## Sex -0.17730737 -17.3 0.010263098 0.000 -0.19853820 ## Swim 0.19426871 14.6 0.013195834 0.000 0.16697105 ## Region 0.02673161 1.1 0.023419425 0.271 -0.02171516 ## IQ -0.01705852 -0.5 0.032839748 0.603 -0.08499272 ## 95% CI (up) ## Height 0.20910441 ## Weight 0.20513839 ## Hairleng -0.15607653 ## Age 0.08057439 ## Income 0.08332148 ## Beer 0.15195189 ## Wine 0.14511150 ## Sex -0.15607653 ## Swim 0.22156638 ## Region 0.07517838 ## IQ 0.05087567 ## ## Degrees of freedom: 23, t-value: 2.07 summary(mjk$coeffs, ncomp = 2, alpha = 0.01) ## Regression coefficients for Shoesize ## ------------------------------------ ## Estimated coefficients t-value SE p-value 99% CI (lo) ## Height 0.19357436 25.6 0.007507308 0.000 0.172498826 ## Weight 0.18935489 24.6 0.007629829 0.000 0.167935399 ## Hairleng -0.17730737 -17.3 0.010263098 0.000 -0.206119327 ## Age 0.02508113 0.9 0.026825735 0.358 -0.050227714 ## Income 0.01291497 0.4 0.034034881 0.703 -0.082632366 ## Beer 0.11441573 6.3 0.018145181 0.000 0.063476112 ## Wine 0.08278735 2.7 0.030127820 0.012 -0.001791551 ## Sex -0.17730737 -17.3 0.010263098 0.000 -0.206119327 ## Swim 0.19426871 14.6 0.013195834 0.000 0.157223579 ## Region 0.02673161 1.1 0.023419425 0.271 -0.039014575 ## IQ -0.01705852 -0.5 0.032839748 0.603 -0.109250717 ## 99% CI (up) ## Height 0.21464989 ## Weight 0.21077438 ## Hairleng -0.14849541 ## Age 0.10038997 ## Income 0.10846230 ## Beer 0.16535534 ## Wine 0.16736626 ## Sex -0.14849541 ## Swim 0.23131385 ## Region 0.09247780 ## IQ 0.07513367 ## ## Degrees of freedom: 23, t-value: 2.81 Function getRegcoeffs() in this case may also return corresponding t-value, standard error, p-value, and confidence interval for each of the coefficient (except intercept) if user specifies a parameter full. The standard error and confidence intervals are also computed for raw, undstandardized, variables (similar to coefficients). show(getRegcoeffs(mjk, ncomp = 2, full = T)) ## Estimated coefficients t-value SE p-value ## Intercept 1.342626e+01 NA NA NA ## Height 7.456695e-02 25.5886755 2.891897e-03 2.116375e-18 ## Weight 4.896328e-02 24.6148053 1.972917e-03 5.012592e-18 ## Hairleng -6.865495e-01 -17.2818695 3.973960e-02 1.137354e-14 ## Age 1.081716e-02 0.9380153 1.156959e-02 3.579832e-01 ## Income 5.642220e-06 0.3857258 1.486897e-05 7.032445e-01 ## Beer 5.010542e-03 6.2692224 7.946214e-04 2.136646e-06 ## Wine 7.511377e-03 2.7258935 2.733526e-03 1.204856e-02 ## Sex -6.865495e-01 -17.2818695 3.973960e-02 1.137354e-14 ## Swim 1.010496e-01 14.6146811 6.863861e-03 3.941369e-13 ## Region 1.035071e-01 1.1283478 9.068204e-02 2.708059e-01 ## IQ -5.277164e-03 -0.5268369 1.015919e-02 6.033516e-01 ## 95% CI (lo) 95% CI (up) ## Intercept NA NA ## Height 6.858461e-02 8.054929e-02 ## Weight 4.488199e-02 5.304457e-02 ## Hairleng -7.687571e-01 -6.043419e-01 ## Age -1.311636e-02 3.475068e-02 ## Income -2.511659e-05 3.640103e-05 ## Beer 3.366742e-03 6.654342e-03 ## Wine 1.856647e-03 1.316611e-02 ## Sex -7.687571e-01 -6.043419e-01 ## Swim 8.685061e-02 1.152486e-01 ## Region -8.408298e-02 2.910972e-01 ## IQ -2.629305e-02 1.573872e-02 ## attr(,&quot;name&quot;) ## [1] &quot;Regression coefficients for Shoesize&quot; It is also possible to change significance level for confidence intervals. show(getRegcoeffs(mjk, ncomp = 2, full = T, alpha = 0.01)) ## Estimated coefficients t-value SE p-value ## Intercept 1.342626e+01 NA NA NA ## Height 7.456695e-02 25.5886755 2.891897e-03 2.116375e-18 ## Weight 4.896328e-02 24.6148053 1.972917e-03 5.012592e-18 ## Hairleng -6.865495e-01 -17.2818695 3.973960e-02 1.137354e-14 ## Age 1.081716e-02 0.9380153 1.156959e-02 3.579832e-01 ## Income 5.642220e-06 0.3857258 1.486897e-05 7.032445e-01 ## Beer 5.010542e-03 6.2692224 7.946214e-04 2.136646e-06 ## Wine 7.511377e-03 2.7258935 2.733526e-03 1.204856e-02 ## Sex -6.865495e-01 -17.2818695 3.973960e-02 1.137354e-14 ## Swim 1.010496e-01 14.6146811 6.863861e-03 3.941369e-13 ## Region 1.035071e-01 1.1283478 9.068204e-02 2.708059e-01 ## IQ -5.277164e-03 -0.5268369 1.015919e-02 6.033516e-01 ## 99% CI (lo) 99% CI (up) ## Intercept NA NA ## Height 6.644843e-02 8.268548e-02 ## Weight 4.342464e-02 5.450192e-02 ## Hairleng -7.981119e-01 -5.749871e-01 ## Age -2.166256e-02 4.329689e-02 ## Income -3.609997e-05 4.738441e-05 ## Beer 2.779773e-03 7.241311e-03 ## Wine -1.625491e-04 1.518530e-02 ## Sex -7.981119e-01 -5.749871e-01 ## Swim 8.178042e-02 1.203188e-01 ## Region -1.510678e-01 3.580821e-01 ## IQ -3.379741e-02 2.324309e-02 ## attr(,&quot;name&quot;) ## [1] &quot;Regression coefficients for Shoesize&quot; The p-values, t-values and standard errors are stored each as a 3-way array similar to regression coefficients. The selection can be made by comparing e.g. p-values with a threshold similar to what we have done with VIP-scores and selectivity ratio. exclcols = mjk$coeffs$p.values[, 2, 1] &gt; 0.05 show(exclcols) ## Height Weight Hairleng Age Income Beer Wine Sex ## FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE ## Swim Region IQ ## FALSE TRUE TRUE Here p.values[, 2, 1] means values for all predictors, model with two components, first y-variable. newm = pls(X.c, y.c, 3, scale = T, cv = 1, exclcols = exclcols) summary(newm) ## ## PLS model (class pls) summary ## ## Performance and validation: ## Number of selected components: 3 ## X cumexpvar Y cumexpvar RMSE Slope Bias RPD ## Cal 97.85 97.79 0.575 0.98 0.0000 6.87 ## CV 96.64 96.49 0.727 0.97 -0.0161 5.44 show(getRegcoeffs(newm)) ## Shoesize ## Intercept 4.952691046 ## Height 0.091079963 ## Weight 0.052708955 ## Hairleng -0.315927643 ## Age 0.000000000 ## Income 0.000000000 ## Beer 0.009320427 ## Wine 0.018524717 ## Sex -0.315927643 ## Swim 0.134354170 ## Region 0.000000000 ## IQ 0.000000000 ## attr(,&quot;exclrows&quot;) ## Age Income Region IQ ## 4 5 10 11 ## attr(,&quot;name&quot;) ## [1] &quot;Regression coefficients for Shoesize&quot; As you can see, the variables Age, Income, Region and IQ have been excluded as they are not related to the Shoesize, which seems to be correct. Variable selection as well as all described above can be also carried out for PLS discriminant analysis (PLS-DA), which can be explained later in one of the next chapters. "],
["simca-classification.html", "SIMCA classification", " SIMCA classification SIMCA (Soft Independent Modelling of Class Analogy) is a simple but efficient one-class classification method mainly based on PCA. The general idea is to create a PCA model using data for samples/objects belonging to a class and classify new objects based on how good the model can fit them. The decision is made using two distances — \\(Q\\) and \\(T^2\\) and corresponding critical limits. Critical limits computed for both distances (or their combination) are used to cut-off the strangers (extreme objects) and accept class members with a pre-define expected ratio of false negatives (\\(\\alpha\\)). All details about which method for critical limits implemented in this package can be found here. It must be noted that version 0.9.0 brings a lot if improvements and new features related to the critical limits calculation and SIMCA classification, so it can be a good idea to read this chapter first. The classification performance can be assessed using true/false positives and negatives and statistics, showing the ability of a classification model to recognize class members (sensitivity or true positive rate) and how good the model is for identifying strangers (specificity or true negative rate). In addition to that, model also calculates a percent of misclassified objects. All statistics are calculated for calibration and validation (if any) results, but one must be aware that specificity can not be computed without objects not belonging to the class and, therefore, calibration and cross-validation results in SIMCA do not have specificity values. It must be also noted that any SIMCA model or result is also a PCA object and all plots, methods, statistics, available for PCA, can be used for SIMCA objects as well. "],
["calibration-and-validation.html", "Calibration and validation", " Calibration and validation The model calibration is similar to PCA, but there are several additional arguments, which are important for classification. First of all it is a class name. Class name is a string, which can be used later e.g. for identifying class members for testing. The second important argument is a level of significance, alpha. This parameter is used for calculation of statistical limits and can be considered as probability for false negatives. The default value is 0.05. Finally the parameter lim.type allows to select the method for compuring critical limits for the distances, as it is described in the PCA chapter. In this chapter as well as for describing other classification methods we will use a famous Iris dataset, available in R. The dataset includes 150 measurements of three Iris species: Setosa, Virginica and Versicola. The measurements are length and width of petals and sepals in cm. Use ?iris for more details. Let’s get the data and split it to calibration and test sets. data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # generate indices for calibration set idx = seq(1, nrow(iris), by = 2) # split the values X.c = iris[idx, 1:4] c.c = iris[idx, 5, drop = F] X.t = iris[-idx, 1:4] c.t = iris[-idx, 5, drop = F] Now, because for calibration we need only objects belonging to a class, we will split the X.c into three matrices — one for each species. The data is ordered by the species, so it can be done relatively easy by taking every 25 rows. X.set = X.c[1:25, ] X.ver = X.c[26:50, ] X.vir = X.c[51:75, ] Let’s start with creating a model for class Versicolor and exploring available statistics and plots. We will use full cross-validation to validate the results. In this case we do not change method for critical limits, so the default option (jm) is used. library(mdatools) m = simca(X.ver, &#39;versicolor&#39;, ncomp = 3, cv = 1) summary(m) ## ## SIMCA model for class &quot;versicolor&quot; summary ## ## Info: ## Method for critical limits: jm ## Significance level (alpha): 0.05 ## Selected number of components: 3 ## ## Expvar Cumexpvar Sens (cal) Expvar (cv) Sens (cv) ## Comp 1 76.44 76.44 0.96 71.85 0.88 ## Comp 2 13.93 90.37 0.92 13.91 0.84 ## Comp 3 8.45 98.82 0.92 12.24 0.84 Let’s look at plots and start with summary plot. plot(m) The plot is very similar to what we seen for PCA model, the only difference is that it shows modelling power instead of loadings. Modelling power is a measure of contribution of each variable to the model and varies from 0 to 1. Usually variables with modelling power below 0.1 are considered as irrelevant. Let’s give a closer look at the residuals plot with different values for alpha (we will keep number of components equal to three in all cases). m1 = simca(X.ver, &#39;versicolor&#39;, ncomp = 3, cv = 1, alpha = 0.01) m2 = simca(X.ver, &#39;versicolor&#39;, ncomp = 3, cv = 1, alpha = 0.05) m3 = simca(X.ver, &#39;versicolor&#39;, ncomp = 3, cv = 1, alpha = 0.10) m4 = simca(X.ver, &#39;versicolor&#39;, ncomp = 3, cv = 1, alpha = 0.15) par(mfrow = c(2, 2)) plotResiduals(m1) plotResiduals(m2) plotResiduals(m3) plotResiduals(m4) As you can see, using alpha = 0.01 reduced number of false negatives to zero, as the acceptance limits became larger, while alpha = 0.15 gives a lot of incorrectly rejected class members. It must be noted, that decreasing alpha will also lead to a larger number of false positives, which we can not see in this case. Predictions and validation with a test set When model is ready one can test it using a new test set with know classes. In this case we will use objects from all three species and be able to see how good the model performs on strangers (and calculate the specificity). In order to do that we will provide both the matrix with predictors, X.t, and a vector with names of the classes for corresponding objects/rows (c.t). The values with known classes in this case can be: a vector with text values (names) a factor using the names as labels a vector with logical values (TRUE for class members and FALSE for strangers) In our case we have a factor. Instead of creating a new model and providing the values as test set we will make predictions instead. res = predict(m, X.t, c.t) summary(res) ## ## Summary for SIMCA one-class classification result ## ## Class name: versicolor ## Number of selected components: 3 ## ## Expvar Cumexpvar TP FP TN FN Spec Sens ## Comp 1 64.27 64.27 23 5 45 2 0.90 0.92 ## Comp 2 1.67 65.95 24 3 47 1 0.94 0.96 ## Comp 3 32.45 98.40 22 3 47 3 0.94 0.88 In this case we see a more detailed statistics with true/false positives and negatives, specificity and sensitivity. The performance statistics can be also shown as plots. par(mfrow = c(2, 2)) plotSpecificity(res) plotSensitivity(res) plotMisclassified(res) plotPerformance(res) The classification results can be shown both graphically and numerically. Here is a prediction plot for the results. par(mfrow = c(2, 1)) plotPredictions(res, show.legend = F) plotPredictions(res, ncomp = 2, show.legend = F) So we can see (the legend has been hidden in order to see all points clearly) that for the model with three components we have three false positives (specificity = 47/50 = 0.94) and three false negatives (sensitivity = 22/25 = 0.88). You can also show the predictions as a matrix with -1 and +1 using method showPredictions() or get the array with predicted class values directly as it is shown in the example below (for first 10 rows, different number of components and the first classification variable). show(res$c.pred[31:40, 1:3, 1]) ## Comp 1 Comp 2 Comp 3 ## 62 1 1 1 ## 64 1 1 1 ## 66 1 1 1 ## 68 1 1 -1 ## 70 1 1 1 ## 72 1 1 1 ## 74 1 1 -1 ## 76 1 1 1 ## 78 1 1 1 ## 80 1 1 1 You can also get and show the confusion matrix (rows correspond to real classes and columns to the predicted class) for an object with SIMCA results (as well as results obtained with any other classification method, e.g. PLS-DA). In this example the matrix also shows 3 false negatives and 3 false positives we have mentioned before. show(getConfusionMatrix(res)) ## versicolor None ## setosa 0 25 ## versicolor 22 3 ## virginica 3 22 Class belonging probabilities In addition to the array with predicted class, the object with SIMCA results also contains an array with class beloning probabilities. The probabilities are calculated depending on how close a particular object is to the the critical limit border. To compute the probability we use the theoretical distribution for Q and T2 distances as for computing critical values (defined by the parameter lim.type). The distribution is used to calculate a p-value — chance to get object with given distance value or larger. The p-value is then compared with signidicance level, \\(\\alpha\\), and the probability, \\(\\pi\\) is calculated as follows: \\[\\pi = 0.5 (p / \\alpha) \\] So if p-value is the same as significance level (which happens when object is lying exactly on the acceptance line) the probability is 0.5. If p-value is e.g. 0.04, \\(\\pi = 0.4\\), or 40%, and the object will be rejected as a stranger (here we assume that the \\(\\alpha = 0.05\\)). If the p-value is e.g. 0.06, \\(\\pi = 0.6\\), or 60%, and the object will be accepted as a member of the class. If p-value is larger than \\(2\\times\\alpha\\) the probability is set to 1. In case of rectangular acceptance area (lim.type = 'jm' or 'chisq') the probability is computed separately for Q and T2 values and the smallest of the two is taken. In case of triangular acceptance area (lim.type = 'ddmoments' or 'ddrobust') the probability is calculated for a combination of the distances. Here is how to show the probability values, that correspond to the predictions shown in the previous code chunk. show(res$p.pred[31:40, 1:3, 1]) ## Comp 1 Comp 2 Comp 3 ## 62 1 1 1.00000000 ## 64 1 1 1.00000000 ## 66 1 1 1.00000000 ## 68 1 1 0.03884308 ## 70 1 1 1.00000000 ## 72 1 1 1.00000000 ## 74 1 1 0.04593059 ## 76 1 1 1.00000000 ## 78 1 1 1.00000000 ## 80 1 1 1.00000000 It is also possible to show the probability values as a plot with method plotProbabilities(): par(mfrow = c(2, 1)) plotProbabilities(res, cgroup = c.t$Species) plotProbabilities(res, ncomp = 2, cgroup = c.t$Species) The plot can be shown for any SIMCA results (including e.g. calibration set or cross-validated results). Extreme plot Extreme plot was proposed as a tool for comparing methods for computing of critical limits as well as for optimization of model parameters, e.g. number of components. The idea of the plot is following. For any given \\(i\\) we can compute \\(\\alpha&#39;\\) as \\(\\alpha&#39; = i / n\\), where \\(n\\) is the number of objects in calibration set, so \\(i\\) is the number of objects outside the acceptance area for the given \\(\\alpha&#39;\\). Then we can count number of objects, \\(m\\), in our data that have p-value below the \\(\\alpha&#39;\\). In ideal case \\(m = i\\), but in fact there will be some small random variation of the \\(m\\) around \\(i\\). This variation can be computed as \\(±2\\sqrt{i (1 - \\alpha)}\\). For example, if the expected number of extreme objects \\(i = 5\\) and \\(n = 50\\), like in our last model, \\(\\alpha = 5/50 = 0.1\\). So we can expect \\(5±2\\sqrt{5 \\times 0.9} ≈ 5 ± 4\\) objects may have the p-value smaller than \\(0.1\\). The plotExtreme() method counts the number of objects from calibration set rejected by the model for \\(i = 1, 2,...,n\\) (and corresponding alpha values) and shows the values vs the \\(i\\). Besides that, the statistical limits. If all points are lying within the limits, the classification model works as expected. Below you will find an example with four extreme plots made for four Versicolor SIMCA models with different methods for estimation of critical limits. Ve = iris[51:100, 1:4] m1 = simca(Ve, &#39;Versicolor&#39;, ncomp = 2, lim.type = &#39;jm&#39;) m2 = simca(Ve, &#39;Versicolor&#39;, ncomp = 2, lim.type = &#39;chisq&#39;) m3 = simca(Ve, &#39;Versicolor&#39;, ncomp = 2, lim.type = &#39;ddmoments&#39;) m4 = simca(Ve, &#39;Versicolor&#39;, ncomp = 2, lim.type = &#39;ddrobust&#39;) par(mfrow = c(2, 2)) plotExtreme(m1, main = &#39;JM&#39;) plotExtreme(m2, main = &#39;Chisq&#39;) plotExtreme(m3, main = &#39;DD moments&#39;) plotExtreme(m4, main = &#39;DD robust&#39;) "],
["multiclass-classification.html", "Multiclass classification", " Multiclass classification Several SIMCA models can be combined to a special object simcam, which is used to make a multiclass classification. Besides this, it also allows calculating distance between individual models and a discrimination power — importance of variables to discriminate between any two classes. Let’s see how it works. First we create three single-class SIMCA models with individual settings, such as number of optimal components and alpha. m.set = simca(X.set, &#39;setosa&#39;, 3, alpha = 0.01) m.set = selectCompNum(m.set, 1) m.vir = simca(X.vir, &#39;virginica&#39;, 3) m.vir = selectCompNum(m.vir, 2) m.ver = simca(X.ver, &#39;versicolor&#39;, 3) m.ver = selectCompNum(m.ver, 1) Then we combine the models into a SIMCAM model object. Summary will show the performance on calibration set, which is a combination of calibration sets for each of the individual models m = simcam(list(m.set, m.vir, m.ver)) summary(m) ## ## SIMCA multiple classes classification (class simcam) ## Nmber of classes: 3 ## Info: ## ## SIMCA model for class &quot;setosa&quot; summary ## ## Info: ## Method for critical limits: jm ## Significance level (alpha): 0.01 ## Selected number of components: 1 ## ## Expvar Cumexpvar Sens (cal) ## Comp 1 73.51 73.51 1 ## Comp 2 14.24 87.76 1 ## Comp 3 10.44 98.20 1 ## ## SIMCA model for class &quot;virginica&quot; summary ## ## Info: ## Method for critical limits: jm ## Significance level (alpha): 0.05 ## Selected number of components: 2 ## ## Expvar Cumexpvar Sens (cal) ## Comp 1 76.16 76.16 0.88 ## Comp 2 14.94 91.10 1.00 ## Comp 3 6.09 97.20 0.96 ## ## SIMCA model for class &quot;versicolor&quot; summary ## ## Info: ## Method for critical limits: jm ## Significance level (alpha): 0.05 ## Selected number of components: 1 ## ## Expvar Cumexpvar Sens (cal) ## Comp 1 76.44 76.44 0.96 ## Comp 2 13.93 90.37 0.92 ## Comp 3 8.45 98.82 0.92 Now we apply the combined model to the test set and look at the predictions. res = predict(m, X.t, c.t) plotPredictions(res) In this case the predictions are shown only for the number of components each model found optimal. The names of classes along y-axis are the individual models. Similarly we can show the predicted values. show(res$c.pred[20:30, 1, 1:3]) ## setosa virginica versicolor ## 40 1 -1 -1 ## 42 -1 -1 -1 ## 44 1 -1 -1 ## 46 1 -1 -1 ## 48 1 -1 -1 ## 50 1 -1 -1 ## 52 -1 -1 1 ## 54 -1 -1 1 ## 56 -1 1 1 ## 58 -1 -1 -1 ## 60 -1 -1 1 Method getConfusionMatrix() is also available in this case. show(getConfusionMatrix(res)) ## setosa virginica versicolor None ## setosa 24 0 0 1 ## versicolor 0 2 23 2 ## virginica 0 21 5 4 There are three additional plots available for multiclass SIMCA model. First of all it is a distance between a selected model and the others. par(mfrow = c(1, 2)) plotModelDistance(m, 1) plotModelDistance(m, 2) The second plot is a discrimination power, mentioned in the beginning of the section. par(mfrow = c(1, 2)) plotDiscriminationPower(m, c(1, 3), show.labels = T) plotDiscriminationPower(m, c(2, 3), show.labels = T) And, finally, a Cooman’s plot showing an orthogonal distance from objects to two selected classes/models. par(mfrow = c(1, 2)) plotCooman(m, c(1, 3), show.labels = T) plotCooman(m, c(2, 3), show.labels = T) "],
["pls-discriminant-analysis.html", "PLS Discriminant Analysis", " PLS Discriminant Analysis PLS Discriminant Analysis (PLS-DA) is a discrimination method based on PLS regression. At some point the idea of PLS-DA is similar to logistic regression — we use PLS for a dummy response variable, y, which is equal to +1 for objects belonging to a class, and -1 for those that do not (in some implementations it can also be 1 and 0 correspondingly). Then a conventional PLS regression model is calibrated and validated, which means that all methods and plots, you already used in PLS, can be used for PLS-DA models and results as well. The extra step in PLS-DA is, actually, classification, which is based on thresholding of predicted y-values. If the predicted value is above 0, a corresponding object is considered as a member of a class and if not — as a stranger. In mdatools this is done automatically using methods plsda() and plsdares(), which inhertit all pls() and plsres() methods. Plus they have something extra to represent classification results, which you have already read about in the chapter devoted to SIMCA. If you have not, it makes sense to do this first, to make the understanding of PLS-DA implementation easier. In this chapter we will describe shortly how PLS-DA implementation works. All examples are based on the well-known Iris dataset, which will be split into two subsets — calibration (75 objects, 25 for each class) and validation (another 75 objects). Two PLS-DA models will be built — one only for virginica class and one for all three classes. "],
["calibration-of-pls-da-model.html", "Calibration of PLS-DA model", " Calibration of PLS-DA model Calibration of PLS-DA model is very similar to conventional PLS with one difference — you need to provide information about class membership of each object instead of a matrix or a vector with response values. This can be done in two different ways. If you have multiple classes, it is always recommended to provide your class membering data as a factor with predefined labels or a vector with class names as text values. The labels/values in this case will be used as class names. It is also acceptable to use numbers as labels but it will make interpretation of results less readable and can possible cause problems with performance statistics calculations. So use names! It is very important that you use the same labels/names for e.g. calibration and test set, because this is the way model will identify which class an object came from. And if you have e.g. a typo in a label value, model will assume that the corresponding object is a stranger. So let’s prepare our data. data(iris) cal.ind = c(1:25, 51:75, 101:125) val.ind = c(26:50, 76:100, 126:150) Xc = iris[cal.ind, 1:4] Xv = iris[val.ind, 1:4] cc.all = iris[cal.ind, 5] cv.all = iris[val.ind, 5] In this case, the fifth column of dataset Iris is already factor, otherwise we have to make it as a factor explicitely. Lets check if it is indeed correct. show(cc.all) ## [1] setosa setosa setosa setosa setosa setosa ## [7] setosa setosa setosa setosa setosa setosa ## [13] setosa setosa setosa setosa setosa setosa ## [19] setosa setosa setosa setosa setosa setosa ## [25] setosa versicolor versicolor versicolor versicolor versicolor ## [31] versicolor versicolor versicolor versicolor versicolor versicolor ## [37] versicolor versicolor versicolor versicolor versicolor versicolor ## [43] versicolor versicolor versicolor versicolor versicolor versicolor ## [49] versicolor versicolor virginica virginica virginica virginica ## [55] virginica virginica virginica virginica virginica virginica ## [61] virginica virginica virginica virginica virginica virginica ## [67] virginica virginica virginica virginica virginica virginica ## [73] virginica virginica virginica ## Levels: setosa versicolor virginica However, for the model with just one class, virginica, we need to prepare the class variable in a different way. In this case it is enought to provide a vector with logical values, where TRUE will correspond to a member and FALSE to a non-member of the class. Here is an example how to do it (we will make two — one for calibration and one for validation subsets). cc.vir = cc.all == &#39;virginica&#39; cv.vir = cv.all == &#39;virginica&#39; show(cc.vir) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [45] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [56] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [67] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE Now we can calibrate the models: m.all = plsda(Xc, cc.all, 3, cv = 1) m.vir = plsda(Xc, cc.vir, 3, cv = 1, classname = &#39;virginica&#39;) You could notice one important difference. In case when parameter c is a vector with logical values you also need to provide a name of the class. If you do not do this, a default name will be used, but it may cause problems when you e.g. validate your model using test set where class membership is a factor as we have in this example. Let’s look at the summary for each of the model. As you can see below, summary for multi class PLS-DA simply shows one set of results for each class. The performance statistics include explained X and Y variance (individual for last used component and cumulative for all of them), values for confusion matrix (True Positives, False Positives, True Negatives, False Negatives) as well as specificity and sensitivity values. summary(m.all) ## ## PLS-DA model (class plsda) summary statistics ## ## Number of selected components: 3 ## Info: ## ## Class #1 (setosa) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Cal 2.19 99.65 4.82 58.05 25 0 50 0 1 1 ## CV 2.35 99.61 4.04 53.59 25 0 50 0 1 1 ## ## Class #2 (versicolor) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Cal 2.19 99.65 4.82 58.05 11 5 45 14 0.90 0.44 ## CV 2.35 99.61 4.04 53.59 10 6 44 15 0.88 0.40 ## ## Class #3 (virginica) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Cal 2.19 99.65 4.82 58.05 24 3 47 1 0.94 0.96 ## CV 2.35 99.61 4.04 53.59 24 3 47 1 0.94 0.96 Dealing with the multi-class PLS-DA model is similar to dealing with PLS2 models, when you have several y-variables. Every time you want to show a plot or results for a particular class, just provide the class number using parameter nc. For example this is how to show summary only for the third class (virginica). summary(m.all, nc = 3) ## ## PLS-DA model (class plsda) summary statistics ## ## Number of selected components: 3 ## Info: ## ## Class #3 (virginica) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Cal 2.19 99.65 4.82 58.05 24 3 47 1 0.94 0.96 ## CV 2.35 99.61 4.04 53.59 24 3 47 1 0.94 0.96 You can also show statistics only for calibration or only for cross-validation parts, in this case you will see details about contribution of every component to the model. summary(m.all$calres, nc = 3) ## ## PLS-DA results (class plsdares) summary: ## Number of selected components: 3 ## ## Class #3 (virginica) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Comp 1 91.97 91.97 46.36 46.36 24 5 45 1 0.90 0.96 ## Comp 2 5.50 97.47 6.88 53.23 21 5 45 4 0.90 0.84 ## Comp 3 2.19 99.65 4.82 58.05 24 3 47 1 0.94 0.96 For one class models, the behaviour is actually similar, but there will be always one set of results — for the corresponding class. Here is the summary. summary(m.vir) ## ## PLS-DA model (class plsda) summary statistics ## ## Number of selected components: 3 ## Info: ## ## Class #1 (virginica) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Cal 4.71 98.53 0.06 61.31 24 3 47 1 0.94 0.96 ## CV 6.13 98.90 2.22 57.15 24 3 47 1 0.94 0.96 Like in SIMCA you can also get a confusion matrix for particular result. Here is an example for multiple classes model. getConfusionMatrix(m.all$calres) ## setosa versicolor virginica None ## setosa 25 0 0 0 ## versicolor 0 11 3 11 ## virginica 0 5 24 0 And for the one-class model. getConfusionMatrix(m.vir$calres) ## virginica None ## None 3 47 ## virginica 24 1 "],
["classification-plots.html", "Classification plots", " Classification plots Most of the plots for visualisation of classification results described in SIMCA chapter can be also used for PLS-DA models and results. Let’s start with classification plots. By default it is shown for cross-validation results (we change position of the legend so it does not hide the points). You can clearly spot for example three false negatives and one false positive in the one-class PLS-DA model for virginica. par(mfrow = c(1, 2)) plotPredictions(m.all, legend.position = &#39;top&#39;) plotPredictions(m.vir, legend.position = &#39;top&#39;) In case of multiple classes model you can select which class to show the predictions for. par(mfrow = c(1, 2)) plotPredictions(m.all, nc = 1, legend.position = &#39;top&#39;) plotPredictions(m.all, nc = 3, legend.position = &#39;top&#39;) "],
["performance-plots.html", "Performance plots", " Performance plots As in SIMCA you can show how sensitivity, specificity and total amount of misclassified samples depending on number of components by using corresponding plots. In case of multiple-classes model you can also provide a class number to show the plot for (by default package will show the plot for overall statistic computed for all classes). par(mfrow = c(3, 2)) plotMisclassified(m.all, nc = 2, legend.position = &#39;top&#39;) plotMisclassified(m.vir, legend.position = &#39;top&#39;) plotSensitivity(m.all, nc = 2, legend.position = &#39;top&#39;) plotSensitivity(m.vir, legend.position = &#39;top&#39;) plotSpecificity(m.all, nc = 2, legend.position = &#39;top&#39;) plotSpecificity(m.vir, legend.position = &#39;top&#39;) As usual, you can also change type of plot to line or scatter-line, change colors, etc. All PLS regression plots, including RMSE, X and Y variance, etc. will also work smoothly with PLS-DA models or results. You can also show regression coefficients like in the example below. To show regression coefficients for particular class you need to provide its number using parameter ny (the reason we use ny and not nc here is that this plot is inherited from PLS model). So in our example both plots show regression coefficients for prediction of virginica objects (left obtained using multiple class model and right — using one class model). par(mfrow = c(1, 2)) plotRegcoeffs(m.all, ncomp = 3, ny = 3) plotRegcoeffs(m.vir) "],
["predictions-for-a-new-data.html", "Predictions for a new data", " Predictions for a new data Again very similar to PLS — just use method predict() and provide at least matrix or data frame with predictors (which should contain the same number of variables/columns). For test set validation you can also provide class reference information similar to what you have used for calibration of PLS-DA models. In case of multiple class model, the reference values should be provided as a factor or vector with class names as text values. Here is an example. res = predict(m.all, Xv, cv.all) summary(res) ## ## PLS-DA results (class plsdares) summary: ## Number of selected components: 3 ## ## Class #1 (setosa) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Comp 1 92.92 92.92 47.01 47.01 25 1 49 0 0.98 1 ## Comp 2 4.56 97.48 10.37 57.39 25 0 50 0 1.00 1 ## Comp 3 1.79 99.27 1.59 58.97 25 0 50 0 1.00 1 ## ## ## Class #2 (versicolor) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Comp 1 92.92 92.92 47.01 47.01 0 0 50 25 1.00 0.0 ## Comp 2 4.56 97.48 10.37 57.39 10 4 46 15 0.92 0.4 ## Comp 3 1.79 99.27 1.59 58.97 10 6 44 15 0.88 0.4 ## ## ## Class #3 (virginica) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Comp 1 92.92 92.92 47.01 47.01 25 4 46 0 0.92 1.00 ## Comp 2 4.56 97.48 10.37 57.39 25 4 46 0 0.92 1.00 ## Comp 3 1.79 99.27 1.59 58.97 24 4 46 1 0.92 0.96 And the corresponding plot with predictions. par(mfrow = c(1, 1)) plotPredictions(res, legend.position = &#39;topleft&#39;) If vector with reference class values contains names of classes model knows nothing about, they will simply be considered as members of non of the known clases (“None”). In case of one-class model, the reference values can be either factor/vector with names or logical values, like the ones used for calibration of the model. Here is an example for each of the cases. res21 = predict(m.vir, Xv, cv.all) summary(res21) ## ## PLS-DA results (class plsdares) summary: ## Number of selected components: 3 ## ## Class #1 (virginica) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Comp 1 92.95 92.95 53.96 53.96 25 4 46 0 0.92 1.00 ## Comp 2 1.62 94.57 6.10 60.06 24 4 46 1 0.92 0.96 ## Comp 3 2.70 97.27 -0.15 59.91 22 4 46 3 0.92 0.88 res22 = predict(m.vir, Xv, cv.vir) summary(res22) ## ## PLS-DA results (class plsdares) summary: ## Number of selected components: 3 ## ## Class #1 (virginica) ## X expvar X cumexpvar Y expvar Y cumexpvar TP FP TN FN Spec Sens ## Comp 1 92.95 92.95 53.96 53.96 25 4 46 0 0.92 1.00 ## Comp 2 1.62 94.57 6.10 60.06 24 4 46 1 0.92 0.96 ## Comp 3 2.70 97.27 -0.15 59.91 22 4 46 3 0.92 0.88 As you can see, statistically results are identical. However, predictions plot will look a bit different for these two cases, as you can see below. par(mfrow = c(2, 1)) plotPredictions(res21, legend.position = &#39;topleft&#39;) plotPredictions(res22, legend.position = &#39;topleft&#39;) And because predict() returns an object with results you can also use most of the plots available for PLS regression results. In the last example below you will find plots for X-residuals and Y-varaince. par(mfrow = c(1, 2)) plotXResiduals(res21) plotYVariance(res22) "]
]
